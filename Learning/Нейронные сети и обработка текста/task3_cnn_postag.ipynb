{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свёрточные нейросети и POS-теггинг\n",
    "\n",
    "POS-теггинг - определение частей речи (снятие частеречной неоднозначности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:42:57.976431Z",
     "start_time": "2019-10-29T19:42:57.959538Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pyconll\n",
    "# !pip install spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:34.549739Z",
     "start_time": "2019-10-29T19:49:32.179692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyconll\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
    "    character_tokenize, pos_corpus_to_tensor, POSTagger\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:08.433599Z",
     "start_time": "2019-10-29T19:46:05.110693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" не является внутренней или внешней\n",
      "командой, исполняемой программой или пакетным файлом.\n",
      "\"wget\" не является внутренней или внешней\n",
      "командой, исполняемой программой или пакетным файлом.\n"
     ]
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.525561Z",
     "start_time": "2019-10-29T19:49:37.315213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('./datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.548127Z",
     "start_time": "2019-10-29T19:49:56.527559Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.916262Z",
     "start_time": "2019-10-29T19:49:56.549806Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:57.251433Z",
     "start_time": "2019-10-29T19:49:56.919818Z"
    }
   },
   "outputs": [],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.124148Z",
     "start_time": "2019-10-29T19:49:57.254191Z"
    }
   },
   "outputs": [],
   "source": [
    "train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
    "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
    "print(\"Количество уникальных символов\", len(char_vocab))\n",
    "print(list(char_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.524125Z",
     "start_time": "2019-10-29T19:49:58.125577Z"
    }
   },
   "outputs": [],
   "source": [
    "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.752672Z",
     "start_time": "2019-10-29T19:49:58.526431Z"
    }
   },
   "outputs": [],
   "source": [
    "train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.754883Z",
     "start_time": "2019-10-29T19:49:40.582Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.756496Z",
     "start_time": "2019-10-29T19:49:40.711Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательная свёрточная архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.316516Z",
     "start_time": "2019-10-29T19:46:17.539Z"
    }
   },
   "outputs": [],
   "source": [
    "class StackedConv1d(nn.Module):\n",
    "    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(layers_n):\n",
    "            layers.append(nn.Sequential(\n",
    "                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне отдельных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.317452Z",
     "start_time": "2019-10-29T19:46:23.135Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        \n",
    "        features = self.backbone(char_embeddings)\n",
    "        \n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "        \n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.318497Z",
     "start_time": "2019-10-29T19:46:23.764Z"
    }
   },
   "outputs": [],
   "source": [
    "single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.319470Z",
     "start_time": "2019-10-29T19:46:25.552Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_single_token_model) = train_eval_loop(single_token_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            F.cross_entropy,\n",
    "                                            lr=5e-3,\n",
    "                                            epoch_n=10,\n",
    "                                            batch_size=64,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=5,\n",
    "                                            max_batches_per_epoch_train=500,\n",
    "                                            max_batches_per_epoch_val=100,\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                       factor=0.5,\n",
    "                                                                                                                       verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.320568Z",
     "start_time": "2019-10-29T19:46:47.579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_single_token_model.state_dict(), './models/single_token_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.321566Z",
     "start_time": "2019-10-29T19:46:47.731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "single_token_model.load_state_dict(torch.load('./models/single_token_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.324276Z",
     "start_time": "2019-10-29T19:46:48.445Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict_with_model(single_token_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(single_token_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне предложений (с учётом контекста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.325744Z",
     "start_time": "2019-10-29T19:46:50.139Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        char_features = self.single_token_backbone(char_embeddings)\n",
    "        \n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.326925Z",
     "start_time": "2019-10-29T19:46:50.310Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.327888Z",
     "start_time": "2019-10-29T19:46:50.737Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model) = train_eval_loop(sentence_level_model,\n",
    "                                              train_dataset,\n",
    "                                              test_dataset,\n",
    "                                              F.cross_entropy,\n",
    "                                              lr=5e-3,\n",
    "                                              epoch_n=10,\n",
    "                                              batch_size=64,\n",
    "                                              device='cuda',\n",
    "                                              early_stopping_patience=5,\n",
    "                                              max_batches_per_epoch_train=500,\n",
    "                                              max_batches_per_epoch_val=100,\n",
    "                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                         factor=0.5,\n",
    "                                                                                                                         verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.542052Z",
     "start_time": "2019-08-29T13:56:16.529110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_sentence_level_model.state_dict(), './models/sentence_level_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.564926Z",
     "start_time": "2019-08-29T13:56:16.544481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "sentence_level_model.load_state_dict(torch.load('./models/sentence_level_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.092139Z",
     "start_time": "2019-08-29T13:56:16.567242Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение полученных теггеров и сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.105418Z",
     "start_time": "2019-08-29T13:56:42.093744Z"
    }
   },
   "outputs": [],
   "source": [
    "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.125540Z",
     "start_time": "2019-08-29T13:56:42.106771Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.148124Z",
     "start_time": "2019-08-29T13:56:42.126930Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.168810Z",
     "start_time": "2019-08-29T13:56:42.149698Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточный модуль своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.193140Z",
     "start_time": "2019-08-29T13:56:42.170233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
    "\n",
    "        batch_size, src_channels, sequence_len = x.shape        \n",
    "        if self.padding > 0:\n",
    "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
    "            x = torch.cat((pad, x, pad), dim=-1)\n",
    "            sequence_len = x.shape[-1]\n",
    "\n",
    "        chunks = []\n",
    "        chunk_size = sequence_len - self.kernel_size + 1\n",
    "        for offset in range(self.kernel_size):\n",
    "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
    "\n",
    "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
    "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
    "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
    "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
    "        return out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.210013Z",
     "start_time": "2019-08-29T13:56:42.194620Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
    "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:00.233326Z",
     "start_time": "2019-08-29T13:56:42.211456Z"
    }
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n",
    "                                                      train_dataset,\n",
    "                                                      test_dataset,\n",
    "                                                      F.cross_entropy,\n",
    "                                                      lr=5e-3,\n",
    "                                                      epoch_n=10,\n",
    "                                                      batch_size=64,\n",
    "                                                      device='cuda',\n",
    "                                                      early_stopping_patience=5,\n",
    "                                                      max_batches_per_epoch_train=500,\n",
    "                                                      max_batches_per_epoch_val=100,\n",
    "                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                                 factor=0.5,\n",
    "                                                                                                                                 verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:39.145214Z",
     "start_time": "2019-08-29T14:06:00.234936Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
