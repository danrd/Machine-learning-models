{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBSrM3NIdJ4r"
   },
   "source": [
    "## Стандартный пайплайн обучения модели\n",
    "\n",
    "pykeen предоставляет стандартный интерфейс-[pipeline](https://pykeen.readthedocs.io/en/stable/api/pykeen.pipeline.pipeline.html#pykeen.pipeline.pipeline) для обучения KG embedding алгоритмов на любых датасетах. \n",
    "\n",
    "В базовом примере мы обучим TransE (в стандартных настройках) на небольшом датасете Kinships, где обучение не займет больше 4 минут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IraMw-50ByRS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\El Tuko\\anaconda3\\envs\\pykeen\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No random seed is specified. Setting to 3753162561.\n",
      "No cuda devices were available. The model runs on CPU\n",
      "C:\\Users\\El Tuko\\anaconda3\\envs\\pykeen\\lib\\site-packages\\pykeen\\nn\\representation.py:369: UserWarning: Directly use Embedding.max_id instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.max_id instead of num_embeddings.\")\n",
      "C:\\Users\\El Tuko\\anaconda3\\envs\\pykeen\\lib\\site-packages\\pykeen\\nn\\representation.py:375: UserWarning: Directly use Embedding.shape instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.shape instead of num_embeddings.\")\n",
      "Training epochs on cpu:   0%|          | 0/300 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 125.00batch/s]\u001b[A\n",
      "Training batches on cpu:  76%|███████▋  | 26/34 [00:00<00:00, 124.77batch/s]\u001b[A\n",
      "Training epochs on cpu:   0%|          | 1/300 [00:00<02:08,  2.33epoch/s, loss=0.00474, prev_loss=nan]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 163.98batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 163.68batch/s]\u001b[A\n",
      "Training epochs on cpu:   1%|          | 2/300 [00:00<01:44,  2.85epoch/s, loss=0.00461, prev_loss=0.00474]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 161.91batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 161.00batch/s]\u001b[A\n",
      "Training epochs on cpu:   1%|          | 3/300 [00:01<01:37,  3.05epoch/s, loss=0.0045, prev_loss=0.00461] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 168.31batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 144.85batch/s]\u001b[A\n",
      "Training epochs on cpu:   1%|▏         | 4/300 [00:01<01:37,  3.04epoch/s, loss=0.0043, prev_loss=0.0045] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 133.34batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 135.87batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏         | 5/300 [00:01<01:37,  3.02epoch/s, loss=0.00425, prev_loss=0.0043]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 153.63batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 159.28batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏         | 6/300 [00:02<01:35,  3.09epoch/s, loss=0.00418, prev_loss=0.00425]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 162.79batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 165.05batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏         | 7/300 [00:02<01:32,  3.17epoch/s, loss=0.00411, prev_loss=0.00418]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 165.04batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 132.26batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▎         | 8/300 [00:02<01:34,  3.07epoch/s, loss=0.00398, prev_loss=0.00411]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 165.05batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 159.67batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▎         | 9/300 [00:02<01:32,  3.15epoch/s, loss=0.00393, prev_loss=0.00398]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▎         | 10/300 [00:03<01:29,  3.23epoch/s, loss=0.00389, prev_loss=0.00393]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 124.00batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 147.97batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▎         | 11/300 [00:03<01:33,  3.09epoch/s, loss=0.00377, prev_loss=0.00389]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 114.28batch/s]\u001b[A\n",
      "Training batches on cpu:  74%|███████▎  | 25/34 [00:00<00:00, 119.21batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▍         | 12/300 [00:03<01:36,  2.97epoch/s, loss=0.00369, prev_loss=0.00377]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 163.46batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 158.10batch/s]\u001b[A\n",
      "Training epochs on cpu:   4%|▍         | 13/300 [00:04<01:34,  3.04epoch/s, loss=0.00366, prev_loss=0.00369]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 153.84batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 155.61batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▍         | 14/300 [00:04<01:33,  3.05epoch/s, loss=0.00362, prev_loss=0.00366]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 137.25batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 133.41batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▌         | 15/300 [00:04<01:37,  2.93epoch/s, loss=0.00353, prev_loss=0.00362]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 166.42batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 148.91batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▌         | 16/300 [00:05<01:36,  2.96epoch/s, loss=0.00346, prev_loss=0.00353]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 154.93batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 161.80batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▌         | 17/300 [00:05<01:32,  3.06epoch/s, loss=0.00342, prev_loss=0.00346]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 135.93batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 148.20batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▌         | 18/300 [00:05<01:32,  3.06epoch/s, loss=0.00337, prev_loss=0.00342]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 166.59batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 165.29batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋         | 19/300 [00:06<01:29,  3.15epoch/s, loss=0.0033, prev_loss=0.00337] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 156.87batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 160.88batch/s]\u001b[A\n",
      "Training epochs on cpu:   7%|▋         | 20/300 [00:06<01:28,  3.17epoch/s, loss=0.00329, prev_loss=0.0033]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 150.03batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 150.23batch/s]\u001b[A\n",
      "Training epochs on cpu:   7%|▋         | 21/300 [00:06<01:30,  3.08epoch/s, loss=0.00322, prev_loss=0.00329]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 156.86batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 152.46batch/s]\u001b[A\n",
      "Training epochs on cpu:   7%|▋         | 22/300 [00:07<01:31,  3.04epoch/s, loss=0.00315, prev_loss=0.00322]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 88.24batch/s]\u001b[A\n",
      "Training batches on cpu:  68%|██████▊   | 23/34 [00:00<00:00, 115.78batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▊         | 23/300 [00:07<01:34,  2.94epoch/s, loss=0.00311, prev_loss=0.00315]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 172.29batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▊         | 24/300 [00:07<01:31,  3.02epoch/s, loss=0.00303, prev_loss=0.00311]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 174.32batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▊         | 25/300 [00:08<01:29,  3.08epoch/s, loss=0.003, prev_loss=0.00303]  \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 104.76batch/s]\u001b[A\n",
      "Training batches on cpu:  65%|██████▍   | 22/34 [00:00<00:00, 57.27batch/s] \u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 65.25batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|▊         | 26/300 [00:08<01:53,  2.41epoch/s, loss=0.00302, prev_loss=0.003]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 116.51batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 106.75batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|▉         | 27/300 [00:09<01:52,  2.43epoch/s, loss=0.00287, prev_loss=0.00302]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.22batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|▉         | 28/300 [00:09<01:42,  2.67epoch/s, loss=0.00289, prev_loss=0.00287]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|▉         | 29/300 [00:09<01:33,  2.90epoch/s, loss=0.00284, prev_loss=0.00289]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 174.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█         | 30/300 [00:10<01:37,  2.76epoch/s, loss=0.00283, prev_loss=0.00284]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 119.18batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 105.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  10%|█         | 31/300 [00:10<01:40,  2.69epoch/s, loss=0.0027, prev_loss=0.00283] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 103.77batch/s]\u001b[A\n",
      "Training batches on cpu:  65%|██████▍   | 22/34 [00:00<00:00, 98.60batch/s] \u001b[A\n",
      "Training epochs on cpu:  11%|█         | 32/300 [00:10<01:40,  2.66epoch/s, loss=0.00269, prev_loss=0.0027]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 168.90batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 150.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  11%|█         | 33/300 [00:11<01:35,  2.79epoch/s, loss=0.00267, prev_loss=0.00269]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 163.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  11%|█▏        | 34/300 [00:11<01:29,  2.98epoch/s, loss=0.00264, prev_loss=0.00267]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▏        | 35/300 [00:11<01:24,  3.14epoch/s, loss=0.00253, prev_loss=0.00264]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▏        | 36/300 [00:12<01:21,  3.25epoch/s, loss=0.00255, prev_loss=0.00253]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 123.81batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 151.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▏        | 37/300 [00:12<01:22,  3.20epoch/s, loss=0.00252, prev_loss=0.00255]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  13%|█▎        | 38/300 [00:12<01:19,  3.30epoch/s, loss=0.00241, prev_loss=0.00252]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 152.48batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 165.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  13%|█▎        | 39/300 [00:13<01:18,  3.34epoch/s, loss=0.00237, prev_loss=0.00241]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  13%|█▎        | 40/300 [00:13<01:22,  3.14epoch/s, loss=0.00237, prev_loss=0.00237]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 163.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▎        | 41/300 [00:13<01:19,  3.24epoch/s, loss=0.0023, prev_loss=0.00237] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 161.91batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▍        | 42/300 [00:13<01:17,  3.33epoch/s, loss=0.00226, prev_loss=0.0023]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 153.85batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 163.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▍        | 43/300 [00:14<01:16,  3.34epoch/s, loss=0.00221, prev_loss=0.00226]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 138.61batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 155.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  15%|█▍        | 44/300 [00:14<01:17,  3.32epoch/s, loss=0.00218, prev_loss=0.00221]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 158.20batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 164.15batch/s]\u001b[A\n",
      "Training epochs on cpu:  15%|█▌        | 45/300 [00:14<01:16,  3.33epoch/s, loss=0.00214, prev_loss=0.00218]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  15%|█▌        | 46/300 [00:15<01:15,  3.35epoch/s, loss=0.00215, prev_loss=0.00214]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  21%|██        | 7/34 [00:00<00:00, 63.64batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 39.14batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 70.49batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▌        | 47/300 [00:15<01:36,  2.62epoch/s, loss=0.00204, prev_loss=0.00215]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 134.61batch/s]\u001b[A\n",
      "Training batches on cpu:  85%|████████▌ | 29/34 [00:00<00:00, 142.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▌        | 48/300 [00:16<01:33,  2.69epoch/s, loss=0.002, prev_loss=0.00204]  \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 158.67batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 149.20batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▋        | 49/300 [00:16<01:29,  2.81epoch/s, loss=0.00201, prev_loss=0.002]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 157.46batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 142.32batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|█▋        | 50/300 [00:16<01:28,  2.84epoch/s, loss=0.00195, prev_loss=0.00201]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 96.49batch/s]\u001b[A\n",
      "Training batches on cpu:  85%|████████▌ | 29/34 [00:00<00:00, 139.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|█▋        | 51/300 [00:17<01:26,  2.87epoch/s, loss=0.00196, prev_loss=0.00195]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 168.32batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 123.49batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|█▋        | 52/300 [00:17<01:30,  2.73epoch/s, loss=0.00196, prev_loss=0.00196]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 105.65batch/s]\u001b[A\n",
      "Training batches on cpu:  74%|███████▎  | 25/34 [00:00<00:00, 124.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|█▊        | 53/300 [00:17<01:28,  2.78epoch/s, loss=0.00194, prev_loss=0.00196]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 138.61batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 115.41batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|█▊        | 54/300 [00:18<01:28,  2.79epoch/s, loss=0.00187, prev_loss=0.00194]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 152.39batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 159.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  18%|█▊        | 55/300 [00:18<01:23,  2.92epoch/s, loss=0.0019, prev_loss=0.00187] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 171.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|█▊        | 56/300 [00:18<01:23,  2.93epoch/s, loss=0.00182, prev_loss=0.0019]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 137.25batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 117.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|█▉        | 57/300 [00:19<01:32,  2.64epoch/s, loss=0.00185, prev_loss=0.00182]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 114.29batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 108.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|█▉        | 58/300 [00:19<01:38,  2.45epoch/s, loss=0.00184, prev_loss=0.00185]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 87.25batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 92.51batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 98.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|█▉        | 59/300 [00:20<01:44,  2.31epoch/s, loss=0.00183, prev_loss=0.00184]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 106.63batch/s]\u001b[A\n",
      "Training batches on cpu:  65%|██████▍   | 22/34 [00:00<00:00, 100.97batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 99.28batch/s] \u001b[A\n",
      "Training epochs on cpu:  20%|██        | 60/300 [00:20<01:47,  2.23epoch/s, loss=0.00182, prev_loss=0.00183]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  24%|██▎       | 8/34 [00:00<00:00, 72.07batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 74.44batch/s]\u001b[A\n",
      "Training batches on cpu:  76%|███████▋  | 26/34 [00:00<00:00, 85.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██        | 61/300 [00:21<01:52,  2.13epoch/s, loss=0.00178, prev_loss=0.00182]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 116.21batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 112.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  21%|██        | 62/300 [00:21<01:48,  2.19epoch/s, loss=0.00175, prev_loss=0.00178]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 122.64batch/s]\u001b[A\n",
      "Training batches on cpu:  76%|███████▋  | 26/34 [00:00<00:00, 121.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  21%|██        | 63/300 [00:22<01:47,  2.20epoch/s, loss=0.00174, prev_loss=0.00175]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 117.60batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 111.79batch/s]\u001b[A\n",
      "Training epochs on cpu:  21%|██▏       | 64/300 [00:22<01:44,  2.26epoch/s, loss=0.00174, prev_loss=0.00174]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 114.29batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 109.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▏       | 65/300 [00:22<01:42,  2.29epoch/s, loss=0.00173, prev_loss=0.00174]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:   9%|▉         | 3/34 [00:00<00:01, 28.57batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 73.99batch/s]\u001b[A\n",
      "Training batches on cpu:  68%|██████▊   | 23/34 [00:00<00:00, 81.14batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 91.67batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▏       | 66/300 [00:23<01:48,  2.15epoch/s, loss=0.00173, prev_loss=0.00173]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 147.06batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 147.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▏       | 67/300 [00:23<01:37,  2.39epoch/s, loss=0.00172, prev_loss=0.00173]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 194.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▎       | 68/300 [00:24<01:27,  2.66epoch/s, loss=0.00171, prev_loss=0.00172]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 106.82batch/s]\u001b[A\n",
      "Training batches on cpu:  79%|███████▉  | 27/34 [00:00<00:00, 134.59batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▎       | 69/300 [00:24<01:24,  2.72epoch/s, loss=0.00167, prev_loss=0.00171]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 166.66batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 157.58batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▎       | 70/300 [00:24<01:20,  2.87epoch/s, loss=0.00165, prev_loss=0.00167]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 156.86batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 89.66batch/s] \u001b[A\n",
      "Training epochs on cpu:  24%|██▎       | 71/300 [00:25<01:31,  2.50epoch/s, loss=0.00167, prev_loss=0.00165]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 156.87batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 156.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▍       | 72/300 [00:25<01:25,  2.68epoch/s, loss=0.00166, prev_loss=0.00167]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 149.53batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 105.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  24%|██▍       | 73/300 [00:26<01:31,  2.47epoch/s, loss=0.00165, prev_loss=0.00166]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 140.19batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 120.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|██▍       | 74/300 [00:26<01:32,  2.43epoch/s, loss=0.00165, prev_loss=0.00165]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 161.70batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 160.79batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|██▌       | 75/300 [00:26<01:29,  2.52epoch/s, loss=0.00164, prev_loss=0.00165]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  21%|██        | 7/34 [00:00<00:00, 66.54batch/s]\u001b[A\n",
      "Training batches on cpu:  68%|██████▊   | 23/34 [00:00<00:00, 118.54batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|██▌       | 76/300 [00:27<01:30,  2.46epoch/s, loss=0.00158, prev_loss=0.00164]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  24%|██▎       | 8/34 [00:00<00:00, 71.42batch/s]\u001b[A\n",
      "Training batches on cpu:  62%|██████▏   | 21/34 [00:00<00:00, 101.03batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|██▌       | 77/300 [00:27<01:31,  2.45epoch/s, loss=0.00163, prev_loss=0.00158]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 149.30batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 139.68batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|██▌       | 78/300 [00:28<01:26,  2.57epoch/s, loss=0.00163, prev_loss=0.00163]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 159.17batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 162.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  26%|██▋       | 79/300 [00:28<01:21,  2.73epoch/s, loss=0.00154, prev_loss=0.00163]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 146.79batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 128.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|██▋       | 80/300 [00:28<01:19,  2.77epoch/s, loss=0.00161, prev_loss=0.00154]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 167.84batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 168.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|██▋       | 81/300 [00:28<01:14,  2.93epoch/s, loss=0.00158, prev_loss=0.00161]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 168.25batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 163.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|██▋       | 82/300 [00:29<01:11,  3.03epoch/s, loss=0.00162, prev_loss=0.00158]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 153.85batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 154.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|██▊       | 83/300 [00:29<01:11,  3.02epoch/s, loss=0.00158, prev_loss=0.00162]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 152.38batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 149.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|██▊       | 84/300 [00:29<01:10,  3.05epoch/s, loss=0.00157, prev_loss=0.00158]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 165.34batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 157.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|██▊       | 85/300 [00:30<01:09,  3.10epoch/s, loss=0.00159, prev_loss=0.00157]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 141.27batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 112.58batch/s]\u001b[A\n",
      "Training epochs on cpu:  29%|██▊       | 86/300 [00:30<01:23,  2.56epoch/s, loss=0.0016, prev_loss=0.00159] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 105.77batch/s]\u001b[A\n",
      "Training batches on cpu:  65%|██████▍   | 22/34 [00:00<00:00, 102.86batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 105.54batch/s]\u001b[A\n",
      "Training epochs on cpu:  29%|██▉       | 87/300 [00:31<01:25,  2.48epoch/s, loss=0.00155, prev_loss=0.0016]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  15%|█▍        | 5/34 [00:00<00:00, 43.33batch/s]\u001b[A\n",
      "Training batches on cpu:  29%|██▉       | 10/34 [00:00<00:00, 34.87batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 50.79batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 42.45batch/s]\u001b[A\n",
      "Training epochs on cpu:  29%|██▉       | 88/300 [00:31<01:46,  1.99epoch/s, loss=0.00158, prev_loss=0.00155]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 144.56batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 86.49batch/s] \u001b[A\n",
      "Training epochs on cpu:  30%|██▉       | 89/300 [00:32<01:43,  2.04epoch/s, loss=0.00154, prev_loss=0.00158]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 154.55batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 126.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███       | 90/300 [00:32<01:33,  2.24epoch/s, loss=0.00156, prev_loss=0.00154]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  29%|██▉       | 10/34 [00:00<00:00, 71.85batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 83.63batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 102.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|███       | 91/300 [00:33<01:38,  2.13epoch/s, loss=0.00151, prev_loss=0.00156]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 144.23batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 63.23batch/s] \u001b[A\n",
      "Training epochs on cpu:  31%|███       | 92/300 [00:33<01:46,  1.95epoch/s, loss=0.00153, prev_loss=0.00151]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  24%|██▎       | 8/34 [00:00<00:00, 63.41batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 48.08batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 40.71batch/s]\u001b[A\n",
      "Training batches on cpu:  79%|███████▉  | 27/34 [00:00<00:00, 47.02batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 42.85batch/s]\u001b[A\n",
      "Training epochs on cpu:  31%|███       | 93/300 [00:34<02:06,  1.63epoch/s, loss=0.00153, prev_loss=0.00153]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 169.60batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 113.71batch/s]\u001b[A\n",
      "Training epochs on cpu:  31%|███▏      | 94/300 [00:35<01:53,  1.82epoch/s, loss=0.0015, prev_loss=0.00153] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 135.06batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 149.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▏      | 95/300 [00:35<01:38,  2.09epoch/s, loss=0.00154, prev_loss=0.0015]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 181.01batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▏      | 96/300 [00:35<01:25,  2.40epoch/s, loss=0.00153, prev_loss=0.00154]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  32%|███▏      | 97/300 [00:36<01:16,  2.66epoch/s, loss=0.00151, prev_loss=0.00153]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 164.01batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 140.59batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▎      | 98/300 [00:36<01:16,  2.63epoch/s, loss=0.00152, prev_loss=0.00151]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 125.23batch/s]\u001b[A\n",
      "Training batches on cpu:  76%|███████▋  | 26/34 [00:00<00:00, 101.53batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▎      | 99/300 [00:36<01:19,  2.52epoch/s, loss=0.0015, prev_loss=0.00152] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 149.01batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 133.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▎      | 100/300 [00:37<01:16,  2.62epoch/s, loss=0.00151, prev_loss=0.0015]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 100.55batch/s]\u001b[A\n",
      "Training batches on cpu:  68%|██████▊   | 23/34 [00:00<00:00, 109.09batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|███▎      | 101/300 [00:37<01:16,  2.59epoch/s, loss=0.00151, prev_loss=0.00151]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|███▍      | 102/300 [00:37<01:12,  2.75epoch/s, loss=0.00149, prev_loss=0.00151]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 165.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|███▍      | 103/300 [00:38<01:07,  2.94epoch/s, loss=0.0015, prev_loss=0.00149] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 169.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  35%|███▍      | 104/300 [00:38<01:05,  3.00epoch/s, loss=0.00147, prev_loss=0.0015]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 87.49batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 87.10batch/s]\u001b[A\n",
      "Training epochs on cpu:  35%|███▌      | 105/300 [00:38<01:07,  2.87epoch/s, loss=0.00144, prev_loss=0.00147]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  35%|███▌      | 106/300 [00:39<01:03,  3.07epoch/s, loss=0.00146, prev_loss=0.00144]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.77batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|███▌      | 107/300 [00:39<01:16,  2.53epoch/s, loss=0.00148, prev_loss=0.00146]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  12%|█▏        | 4/34 [00:00<00:00, 37.04batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 78.35batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 117.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|███▌      | 108/300 [00:40<01:19,  2.42epoch/s, loss=0.00145, prev_loss=0.00148]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  21%|██        | 7/34 [00:00<00:00, 61.40batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 68.44batch/s]\u001b[A\n",
      "Training batches on cpu:  79%|███████▉  | 27/34 [00:00<00:00, 88.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|███▋      | 109/300 [00:40<01:27,  2.19epoch/s, loss=0.00142, prev_loss=0.00145]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 134.62batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 133.86batch/s]\u001b[A\n",
      "Training epochs on cpu:  37%|███▋      | 110/300 [00:41<01:19,  2.39epoch/s, loss=0.00144, prev_loss=0.00142]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 164.93batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 149.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  37%|███▋      | 111/300 [00:41<01:15,  2.52epoch/s, loss=0.00145, prev_loss=0.00144]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 133.73batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 145.78batch/s]\u001b[A\n",
      "Training epochs on cpu:  37%|███▋      | 112/300 [00:41<01:11,  2.64epoch/s, loss=0.00145, prev_loss=0.00145]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 151.63batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 152.93batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|███▊      | 113/300 [00:42<01:06,  2.80epoch/s, loss=0.00142, prev_loss=0.00145]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|███▊      | 114/300 [00:42<01:01,  3.02epoch/s, loss=0.00145, prev_loss=0.00142]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.82batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|███▊      | 115/300 [00:42<00:57,  3.21epoch/s, loss=0.00144, prev_loss=0.00145]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.04batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|███▊      | 116/300 [00:42<00:56,  3.28epoch/s, loss=0.00142, prev_loss=0.00144]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|███▉      | 117/300 [00:43<00:53,  3.42epoch/s, loss=0.00145, prev_loss=0.00142]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 148.31batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 165.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|███▉      | 118/300 [00:43<00:53,  3.43epoch/s, loss=0.00144, prev_loss=0.00145]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.79batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|███▉      | 119/300 [00:43<00:51,  3.52epoch/s, loss=0.00146, prev_loss=0.00144]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 145.46batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 105.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████      | 120/300 [00:44<00:56,  3.20epoch/s, loss=0.00141, prev_loss=0.00146]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  40%|████      | 121/300 [00:44<00:54,  3.31epoch/s, loss=0.00142, prev_loss=0.00141]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.96batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████      | 122/300 [00:44<00:52,  3.41epoch/s, loss=0.00143, prev_loss=0.00142]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 179.56batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████      | 123/300 [00:44<00:50,  3.50epoch/s, loss=0.00143, prev_loss=0.00143]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 161.82batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████▏     | 124/300 [00:45<00:50,  3.48epoch/s, loss=0.0014, prev_loss=0.00143] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 181.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 125/300 [00:45<00:49,  3.55epoch/s, loss=0.00144, prev_loss=0.0014]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 178.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 126/300 [00:45<00:48,  3.61epoch/s, loss=0.00145, prev_loss=0.00144]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 185.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 127/300 [00:46<00:47,  3.66epoch/s, loss=0.00139, prev_loss=0.00145]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 149.21batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 167.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  43%|████▎     | 128/300 [00:46<00:47,  3.59epoch/s, loss=0.00143, prev_loss=0.00139]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 175.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  43%|████▎     | 129/300 [00:46<00:47,  3.61epoch/s, loss=0.00142, prev_loss=0.00143]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 180.00batch/s]\u001b[A\n",
      "Training epochs on cpu:  43%|████▎     | 130/300 [00:46<00:46,  3.65epoch/s, loss=0.00141, prev_loss=0.00142]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▎     | 131/300 [00:47<00:45,  3.70epoch/s, loss=0.00143, prev_loss=0.00141]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  15%|█▍        | 5/34 [00:00<00:00, 45.45batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 86.11batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 121.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 132/300 [00:47<00:51,  3.23epoch/s, loss=0.0014, prev_loss=0.00143] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 164.31batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 160.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 133/300 [00:47<00:51,  3.26epoch/s, loss=0.00138, prev_loss=0.0014]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 168.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|████▍     | 134/300 [00:48<00:50,  3.28epoch/s, loss=0.00141, prev_loss=0.00138]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 166.91batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 156.66batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|████▌     | 135/300 [00:48<00:49,  3.30epoch/s, loss=0.00137, prev_loss=0.00141]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 174.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|████▌     | 136/300 [00:48<00:49,  3.32epoch/s, loss=0.00136, prev_loss=0.00137]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 85.38batch/s]\u001b[A\n",
      "Training batches on cpu:  65%|██████▍   | 22/34 [00:00<00:00, 107.35batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▌     | 137/300 [00:49<00:53,  3.02epoch/s, loss=0.00137, prev_loss=0.00136]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 89.91batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 71.91batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 89.44batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▌     | 138/300 [00:49<01:01,  2.62epoch/s, loss=0.00139, prev_loss=0.00137]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 84.91batch/s]\u001b[A\n",
      "Training batches on cpu:  65%|██████▍   | 22/34 [00:00<00:00, 106.51batch/s]\u001b[A\n",
      "Training epochs on cpu:  46%|████▋     | 139/300 [00:50<01:03,  2.55epoch/s, loss=0.00134, prev_loss=0.00139]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 110.09batch/s]\u001b[A\n",
      "Training batches on cpu:  74%|███████▎  | 25/34 [00:00<00:00, 118.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  47%|████▋     | 140/300 [00:50<01:05,  2.46epoch/s, loss=0.00137, prev_loss=0.00134]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 112.15batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 114.03batch/s]\u001b[A\n",
      "Training epochs on cpu:  47%|████▋     | 141/300 [00:50<01:05,  2.44epoch/s, loss=0.00137, prev_loss=0.00137]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  24%|██▎       | 8/34 [00:00<00:00, 76.92batch/s]\u001b[A\n",
      "Training batches on cpu:  62%|██████▏   | 21/34 [00:00<00:00, 102.80batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 78.55batch/s] \u001b[A\n",
      "Training epochs on cpu:  47%|████▋     | 142/300 [00:51<01:10,  2.23epoch/s, loss=0.00136, prev_loss=0.00137]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  18%|█▊        | 6/34 [00:00<00:00, 47.35batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 59.43batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 75.15batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 143/300 [00:51<01:15,  2.09epoch/s, loss=0.00139, prev_loss=0.00136]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 103.22batch/s]\u001b[A\n",
      "Training batches on cpu:  65%|██████▍   | 22/34 [00:00<00:00, 94.48batch/s] \u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 100.59batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 144/300 [00:52<01:13,  2.13epoch/s, loss=0.00134, prev_loss=0.00139]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 119.94batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 40.92batch/s] \u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 145/300 [00:53<01:25,  1.82epoch/s, loss=0.00135, prev_loss=0.00134]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:   6%|▌         | 2/34 [00:00<00:01, 18.35batch/s]\u001b[A\n",
      "Training batches on cpu:  24%|██▎       | 8/34 [00:00<00:00, 41.78batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 56.81batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 63.40batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 71.30batch/s]\u001b[A\n",
      "Training epochs on cpu:  49%|████▊     | 146/300 [00:53<01:29,  1.72epoch/s, loss=0.00133, prev_loss=0.00135]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  49%|████▉     | 147/300 [00:54<01:14,  2.04epoch/s, loss=0.00133, prev_loss=0.00133]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  49%|████▉     | 148/300 [00:54<01:04,  2.36epoch/s, loss=0.00134, prev_loss=0.00133]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 180.00batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|████▉     | 149/300 [00:54<00:56,  2.65epoch/s, loss=0.00134, prev_loss=0.00134]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 156.86batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 150/300 [00:54<00:53,  2.83epoch/s, loss=0.00139, prev_loss=0.00134]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 196.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 151/300 [00:55<00:48,  3.07epoch/s, loss=0.00138, prev_loss=0.00139]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 99.10batch/s]\u001b[A\n",
      "Training batches on cpu:  62%|██████▏   | 21/34 [00:00<00:00, 82.56batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 101.12batch/s]\u001b[A\n",
      "Training epochs on cpu:  51%|█████     | 152/300 [00:55<00:53,  2.77epoch/s, loss=0.00135, prev_loss=0.00138]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 133.33batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 114.87batch/s]\u001b[A\n",
      "Training epochs on cpu:  51%|█████     | 153/300 [00:56<01:01,  2.39epoch/s, loss=0.00134, prev_loss=0.00135]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 185.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  51%|█████▏    | 154/300 [00:56<00:57,  2.55epoch/s, loss=0.00132, prev_loss=0.00134]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 102.80batch/s]\u001b[A\n",
      "Training batches on cpu:  68%|██████▊   | 23/34 [00:00<00:00, 106.93batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 105.96batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▏    | 155/300 [00:56<00:59,  2.45epoch/s, loss=0.00139, prev_loss=0.00132]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 113.82batch/s]\u001b[A\n",
      "Training batches on cpu:  85%|████████▌ | 29/34 [00:00<00:00, 132.55batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▏    | 156/300 [00:57<00:58,  2.46epoch/s, loss=0.00131, prev_loss=0.00139]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 112.15batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 137.93batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▏    | 157/300 [00:57<00:55,  2.58epoch/s, loss=0.00133, prev_loss=0.00131]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 164.84batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 150.77batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|█████▎    | 158/300 [00:58<00:52,  2.71epoch/s, loss=0.00135, prev_loss=0.00133]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 145.63batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 120.64batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|█████▎    | 159/300 [00:58<00:54,  2.58epoch/s, loss=0.00132, prev_loss=0.00135]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 172.04batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|█████▎    | 160/300 [00:58<00:51,  2.71epoch/s, loss=0.00134, prev_loss=0.00132]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 88.91batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 100.52batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 80.61batch/s] \u001b[A\n",
      "Training epochs on cpu:  54%|█████▎    | 161/300 [00:59<00:58,  2.38epoch/s, loss=0.00129, prev_loss=0.00134]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 129.20batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 157.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  54%|█████▍    | 162/300 [00:59<00:54,  2.51epoch/s, loss=0.00131, prev_loss=0.00129]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 118.66batch/s]\u001b[A\n",
      "Training batches on cpu:  79%|███████▉  | 27/34 [00:00<00:00, 132.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  54%|█████▍    | 163/300 [00:59<00:52,  2.61epoch/s, loss=0.00132, prev_loss=0.00131]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  29%|██▉       | 10/34 [00:00<00:00, 97.86batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 57.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|█████▍    | 164/300 [01:00<00:57,  2.36epoch/s, loss=0.00131, prev_loss=0.00132]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  32%|███▏      | 11/34 [00:00<00:00, 108.91batch/s]\u001b[A\n",
      "Training batches on cpu:  85%|████████▌ | 29/34 [00:00<00:00, 147.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|█████▌    | 165/300 [01:00<00:53,  2.54epoch/s, loss=0.00129, prev_loss=0.00131]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|█████▌    | 166/300 [01:01<00:47,  2.82epoch/s, loss=0.00134, prev_loss=0.00129]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 155.34batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 166.16batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████▌    | 167/300 [01:01<00:45,  2.94epoch/s, loss=0.00128, prev_loss=0.00134]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.52batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████▌    | 168/300 [01:01<00:42,  3.13epoch/s, loss=0.0013, prev_loss=0.00128] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.15batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████▋    | 169/300 [01:01<00:40,  3.26epoch/s, loss=0.0013, prev_loss=0.0013] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|█████▋    | 170/300 [01:02<00:38,  3.37epoch/s, loss=0.00129, prev_loss=0.0013]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 171.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|█████▋    | 171/300 [01:02<00:38,  3.39epoch/s, loss=0.0013, prev_loss=0.00129]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 179.83batch/s]\u001b[A\n",
      "Training epochs on cpu:  57%|█████▋    | 172/300 [01:02<00:36,  3.50epoch/s, loss=0.00135, prev_loss=0.0013]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▊    | 173/300 [01:03<00:35,  3.56epoch/s, loss=0.00131, prev_loss=0.00135]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 137.84batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 152.19batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▊    | 174/300 [01:03<00:36,  3.44epoch/s, loss=0.00131, prev_loss=0.00131]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 171.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▊    | 175/300 [01:03<00:37,  3.30epoch/s, loss=0.00124, prev_loss=0.00131]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 175.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|█████▊    | 176/300 [01:03<00:36,  3.36epoch/s, loss=0.00129, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 153.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|█████▉    | 177/300 [01:04<00:36,  3.39epoch/s, loss=0.00128, prev_loss=0.00129]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 185.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|█████▉    | 178/300 [01:04<00:34,  3.49epoch/s, loss=0.00132, prev_loss=0.00128]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 143.11batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 133.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|█████▉    | 179/300 [01:04<00:37,  3.26epoch/s, loss=0.00133, prev_loss=0.00132]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 172.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|██████    | 180/300 [01:05<00:36,  3.32epoch/s, loss=0.00126, prev_loss=0.00133]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  60%|██████    | 181/300 [01:05<00:34,  3.41epoch/s, loss=0.00131, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|██████    | 182/300 [01:05<00:33,  3.49epoch/s, loss=0.00128, prev_loss=0.00131]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 166.17batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|██████    | 183/300 [01:06<00:33,  3.53epoch/s, loss=0.00128, prev_loss=0.00128]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 185.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|██████▏   | 184/300 [01:06<00:32,  3.58epoch/s, loss=0.00129, prev_loss=0.00128]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 168.10batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▏   | 185/300 [01:06<00:31,  3.61epoch/s, loss=0.0013, prev_loss=0.00129] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 181.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▏   | 186/300 [01:06<00:31,  3.57epoch/s, loss=0.00128, prev_loss=0.0013]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 189.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▏   | 187/300 [01:07<00:31,  3.63epoch/s, loss=0.00126, prev_loss=0.00128]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  63%|██████▎   | 188/300 [01:07<00:31,  3.59epoch/s, loss=0.00124, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 181.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  63%|██████▎   | 189/300 [01:07<00:30,  3.65epoch/s, loss=0.00127, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 190.58batch/s]\u001b[A\n",
      "Training epochs on cpu:  63%|██████▎   | 190/300 [01:07<00:30,  3.63epoch/s, loss=0.00124, prev_loss=0.00127]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|██████▎   | 191/300 [01:08<00:29,  3.67epoch/s, loss=0.00126, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|██████▍   | 192/300 [01:08<00:30,  3.50epoch/s, loss=0.00126, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 147.06batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 152.83batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|██████▍   | 193/300 [01:08<00:31,  3.39epoch/s, loss=0.00128, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 155.34batch/s]\u001b[A\n",
      "Training epochs on cpu:  65%|██████▍   | 194/300 [01:09<00:30,  3.45epoch/s, loss=0.00125, prev_loss=0.00128]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 174.50batch/s]\u001b[A\n",
      "Training epochs on cpu:  65%|██████▌   | 195/300 [01:09<00:30,  3.46epoch/s, loss=0.00123, prev_loss=0.00125]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.86batch/s]\u001b[A\n",
      "Training epochs on cpu:  65%|██████▌   | 196/300 [01:09<00:29,  3.56epoch/s, loss=0.00126, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.96batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|██████▌   | 197/300 [01:09<00:28,  3.57epoch/s, loss=0.00123, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|██████▌   | 198/300 [01:10<00:28,  3.60epoch/s, loss=0.00126, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 171.60batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|██████▋   | 199/300 [01:10<00:27,  3.65epoch/s, loss=0.00123, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 175.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|██████▋   | 200/300 [01:10<00:27,  3.65epoch/s, loss=0.00124, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|██████▋   | 201/300 [01:11<00:27,  3.65epoch/s, loss=0.00128, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|██████▋   | 202/300 [01:11<00:27,  3.55epoch/s, loss=0.00124, prev_loss=0.00128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|██████▊   | 203/300 [01:11<00:27,  3.57epoch/s, loss=0.00126, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 174.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|██████▊   | 204/300 [01:11<00:26,  3.61epoch/s, loss=0.00126, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 146.86batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 163.52batch/s]\u001b[A\n",
      "Training epochs on cpu:  68%|██████▊   | 205/300 [01:12<00:26,  3.54epoch/s, loss=0.00121, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 185.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  69%|██████▊   | 206/300 [01:12<00:26,  3.59epoch/s, loss=0.00125, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  69%|██████▉   | 207/300 [01:12<00:25,  3.64epoch/s, loss=0.00124, prev_loss=0.00125]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 183.49batch/s]\u001b[A\n",
      "Training epochs on cpu:  69%|██████▉   | 208/300 [01:12<00:24,  3.68epoch/s, loss=0.00123, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 191.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|██████▉   | 209/300 [01:13<00:25,  3.63epoch/s, loss=0.00124, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 162.37batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 162.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|███████   | 210/300 [01:13<00:25,  3.54epoch/s, loss=0.00125, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 177.94batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|███████   | 211/300 [01:13<00:24,  3.60epoch/s, loss=0.00119, prev_loss=0.00125]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 189.55batch/s]\u001b[A\n",
      "Training epochs on cpu:  71%|███████   | 212/300 [01:14<00:24,  3.63epoch/s, loss=0.00119, prev_loss=0.00119]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 190.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  71%|███████   | 213/300 [01:14<00:24,  3.61epoch/s, loss=0.00123, prev_loss=0.00119]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  71%|███████▏  | 214/300 [01:14<00:25,  3.37epoch/s, loss=0.00121, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 181.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|███████▏  | 215/300 [01:14<00:24,  3.48epoch/s, loss=0.00126, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 189.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|███████▏  | 216/300 [01:15<00:23,  3.58epoch/s, loss=0.00121, prev_loss=0.00126]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 85.97batch/s]\u001b[A\n",
      "Training batches on cpu:  79%|███████▉  | 27/34 [00:00<00:00, 137.99batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|███████▏  | 217/300 [01:15<00:24,  3.41epoch/s, loss=0.00123, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 183.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|███████▎  | 218/300 [01:15<00:23,  3.49epoch/s, loss=0.00121, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 174.55batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|███████▎  | 219/300 [01:16<00:22,  3.56epoch/s, loss=0.00123, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|███████▎  | 220/300 [01:16<00:22,  3.57epoch/s, loss=0.00121, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 162.90batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|███████▎  | 221/300 [01:16<00:22,  3.57epoch/s, loss=0.00119, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 187.81batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|███████▍  | 222/300 [01:16<00:21,  3.57epoch/s, loss=0.00121, prev_loss=0.00119]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 158.88batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 157.71batch/s]\u001b[A\n",
      "Training epochs on cpu:  74%|███████▍  | 223/300 [01:17<00:22,  3.49epoch/s, loss=0.00124, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 145.63batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 91.17batch/s] \u001b[A\n",
      "Training epochs on cpu:  75%|███████▍  | 224/300 [01:17<00:25,  3.04epoch/s, loss=0.00123, prev_loss=0.00124]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  75%|███████▌  | 225/300 [01:17<00:23,  3.20epoch/s, loss=0.0012, prev_loss=0.00123] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.80batch/s]\u001b[A\n",
      "Training epochs on cpu:  75%|███████▌  | 226/300 [01:18<00:22,  3.35epoch/s, loss=0.00121, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 172.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|███████▌  | 227/300 [01:18<00:21,  3.43epoch/s, loss=0.00121, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 133.29batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 146.72batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|███████▌  | 228/300 [01:18<00:21,  3.32epoch/s, loss=0.0012, prev_loss=0.00121] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 134.61batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 134.62batch/s]\u001b[A\n",
      "Training epochs on cpu:  76%|███████▋  | 229/300 [01:19<00:23,  3.06epoch/s, loss=0.00121, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:   3%|▎         | 1/34 [00:00<00:05,  6.14batch/s]\u001b[A\n",
      "Training batches on cpu:   9%|▉         | 3/34 [00:00<00:02, 12.44batch/s]\u001b[A\n",
      "Training batches on cpu:  29%|██▉       | 10/34 [00:00<00:00, 34.47batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 34.49batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 35.64batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 53.15batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 52.53batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|███████▋  | 230/300 [01:20<00:36,  1.93epoch/s, loss=0.00118, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:   9%|▉         | 3/34 [00:00<00:01, 24.39batch/s]\u001b[A\n",
      "Training batches on cpu:  18%|█▊        | 6/34 [00:00<00:01, 25.62batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 26.86batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 37.56batch/s]\u001b[A\n",
      "Training batches on cpu:  62%|██████▏   | 21/34 [00:00<00:00, 42.33batch/s]\u001b[A\n",
      "Training batches on cpu:  76%|███████▋  | 26/34 [00:00<00:00, 42.59batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 53.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|███████▋  | 231/300 [01:21<00:44,  1.56epoch/s, loss=0.00122, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 82.35batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 80.32batch/s]\u001b[A\n",
      "Training batches on cpu:  79%|███████▉  | 27/34 [00:00<00:00, 78.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|███████▋  | 232/300 [01:21<00:41,  1.65epoch/s, loss=0.00123, prev_loss=0.00122]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 118.29batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 111.63batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████▊  | 233/300 [01:22<00:37,  1.79epoch/s, loss=0.00123, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 122.64batch/s]\u001b[A\n",
      "Training batches on cpu:  76%|███████▋  | 26/34 [00:00<00:00, 115.37batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████▊  | 234/300 [01:22<00:33,  1.95epoch/s, loss=0.00118, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 118.76batch/s]\u001b[A\n",
      "Training batches on cpu:  71%|███████   | 24/34 [00:00<00:00, 117.42batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████▊  | 235/300 [01:22<00:31,  2.09epoch/s, loss=0.00122, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  26%|██▋       | 9/34 [00:00<00:00, 88.12batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 77.89batch/s]\u001b[A\n",
      "Training batches on cpu:  85%|████████▌ | 29/34 [00:00<00:00, 82.39batch/s]\u001b[A\n",
      "Training epochs on cpu:  79%|███████▊  | 236/300 [01:23<00:31,  2.04epoch/s, loss=0.0012, prev_loss=0.00122] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 124.66batch/s]\u001b[A\n",
      "Training batches on cpu:  76%|███████▋  | 26/34 [00:00<00:00, 121.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  79%|███████▉  | 237/300 [01:23<00:28,  2.21epoch/s, loss=0.00123, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  79%|███████▉  | 238/300 [01:24<00:24,  2.51epoch/s, loss=0.00119, prev_loss=0.00123]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 157.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▉  | 239/300 [01:24<00:22,  2.76epoch/s, loss=0.00117, prev_loss=0.00119]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 171.43batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|████████  | 240/300 [01:24<00:20,  2.97epoch/s, loss=0.0012, prev_loss=0.00117] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.21batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|████████  | 241/300 [01:24<00:18,  3.15epoch/s, loss=0.00121, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|████████  | 242/300 [01:25<00:17,  3.30epoch/s, loss=0.00117, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 158.42batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 140.42batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|████████  | 243/300 [01:25<00:17,  3.25epoch/s, loss=0.00118, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.42batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|████████▏ | 244/300 [01:25<00:16,  3.40epoch/s, loss=0.00121, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 183.89batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|████████▏ | 245/300 [01:25<00:15,  3.50epoch/s, loss=0.00115, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 189.29batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|████████▏ | 246/300 [01:26<00:15,  3.54epoch/s, loss=0.0012, prev_loss=0.00115] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  47%|████▋     | 16/34 [00:00<00:00, 158.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  82%|████████▏ | 247/300 [01:26<00:14,  3.55epoch/s, loss=0.00122, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 183.36batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|████████▎ | 248/300 [01:26<00:14,  3.60epoch/s, loss=0.00123, prev_loss=0.00122]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|████████▎ | 249/300 [01:27<00:13,  3.68epoch/s, loss=0.0012, prev_loss=0.00123] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.07batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|████████▎ | 250/300 [01:27<00:14,  3.51epoch/s, loss=0.00122, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 190.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|████████▎ | 251/300 [01:27<00:13,  3.62epoch/s, loss=0.00117, prev_loss=0.00122]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 177.97batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|████████▍ | 252/300 [01:27<00:13,  3.65epoch/s, loss=0.0012, prev_loss=0.00117] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 185.33batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|████████▍ | 253/300 [01:28<00:12,  3.67epoch/s, loss=0.0012, prev_loss=0.0012] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.07batch/s]\u001b[A\n",
      "Training epochs on cpu:  85%|████████▍ | 254/300 [01:28<00:12,  3.62epoch/s, loss=0.00115, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  85%|████████▌ | 255/300 [01:28<00:12,  3.69epoch/s, loss=0.00118, prev_loss=0.00115]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 164.84batch/s]\u001b[A\n",
      "Training epochs on cpu:  85%|████████▌ | 256/300 [01:28<00:11,  3.69epoch/s, loss=0.00119, prev_loss=0.00118]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 194.18batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|████████▌ | 257/300 [01:29<00:11,  3.76epoch/s, loss=0.0012, prev_loss=0.00119] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 142.10batch/s]\u001b[A\n",
      "Training batches on cpu:  88%|████████▊ | 30/34 [00:00<00:00, 125.76batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|████████▌ | 258/300 [01:29<00:12,  3.48epoch/s, loss=0.00116, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 188.12batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|████████▋ | 259/300 [01:29<00:11,  3.58epoch/s, loss=0.00121, prev_loss=0.00116]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.27batch/s]\u001b[A\n",
      "Training epochs on cpu:  87%|████████▋ | 260/300 [01:30<00:11,  3.48epoch/s, loss=0.0012, prev_loss=0.00121] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 133.33batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 159.31batch/s]\u001b[A\n",
      "Training epochs on cpu:  87%|████████▋ | 261/300 [01:30<00:11,  3.42epoch/s, loss=0.00118, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  87%|████████▋ | 262/300 [01:30<00:10,  3.50epoch/s, loss=0.0012, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 186.92batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|████████▊ | 263/300 [01:30<00:10,  3.59epoch/s, loss=0.00118, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|████████▊ | 264/300 [01:31<00:10,  3.56epoch/s, loss=0.00116, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|████████▊ | 265/300 [01:31<00:09,  3.55epoch/s, loss=0.00117, prev_loss=0.00116]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.99batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|████████▊ | 266/300 [01:31<00:09,  3.61epoch/s, loss=0.00121, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 163.69batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|████████▉ | 267/300 [01:32<00:09,  3.61epoch/s, loss=0.00121, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 176.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|████████▉ | 268/300 [01:32<00:08,  3.64epoch/s, loss=0.00117, prev_loss=0.00121]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.17batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|████████▉ | 269/300 [01:32<00:08,  3.61epoch/s, loss=0.00115, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 174.13batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|█████████ | 270/300 [01:32<00:08,  3.63epoch/s, loss=0.00119, prev_loss=0.00115]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 190.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  90%|█████████ | 271/300 [01:33<00:07,  3.68epoch/s, loss=0.00119, prev_loss=0.00119]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 147.31batch/s]\u001b[A\n",
      "Training batches on cpu:  97%|█████████▋| 33/34 [00:00<00:00, 164.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|█████████ | 272/300 [01:33<00:07,  3.60epoch/s, loss=0.0012, prev_loss=0.00119] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  59%|█████▉    | 20/34 [00:00<00:00, 192.04batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|█████████ | 273/300 [01:33<00:07,  3.60epoch/s, loss=0.00117, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.28batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|█████████▏| 274/300 [01:33<00:07,  3.63epoch/s, loss=0.00117, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|█████████▏| 275/300 [01:34<00:06,  3.67epoch/s, loss=0.00115, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 165.05batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|█████████▏| 276/300 [01:34<00:06,  3.63epoch/s, loss=0.00115, prev_loss=0.00115]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.08batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|█████████▏| 277/300 [01:34<00:06,  3.56epoch/s, loss=0.00118, prev_loss=0.00115]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 182.70batch/s]\u001b[A\n",
      "Training epochs on cpu:  93%|█████████▎| 278/300 [01:35<00:06,  3.59epoch/s, loss=0.00116, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  38%|███▊      | 13/34 [00:00<00:00, 123.81batch/s]\u001b[A\n",
      "Training batches on cpu:  85%|████████▌ | 29/34 [00:00<00:00, 141.41batch/s]\u001b[A\n",
      "Training epochs on cpu:  93%|█████████▎| 279/300 [01:35<00:06,  3.41epoch/s, loss=0.00119, prev_loss=0.00116]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 162.35batch/s]\u001b[A\n",
      "Training batches on cpu: 100%|██████████| 34/34 [00:00<00:00, 164.07batch/s]\u001b[A\n",
      "Training epochs on cpu:  93%|█████████▎| 280/300 [01:35<00:05,  3.42epoch/s, loss=0.00117, prev_loss=0.00119]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 134.67batch/s]\u001b[A\n",
      "Training batches on cpu:  82%|████████▏ | 28/34 [00:00<00:00, 133.88batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|█████████▎| 281/300 [01:36<00:05,  3.27epoch/s, loss=0.00113, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 163.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|█████████▍| 282/300 [01:36<00:05,  3.35epoch/s, loss=0.00115, prev_loss=0.00113]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 180.95batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|█████████▍| 283/300 [01:36<00:04,  3.46epoch/s, loss=0.0012, prev_loss=0.00115] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|█████████▍| 284/300 [01:36<00:04,  3.52epoch/s, loss=0.00112, prev_loss=0.0012]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.48batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|█████████▌| 285/300 [01:37<00:04,  3.53epoch/s, loss=0.00118, prev_loss=0.00112]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 188.12batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|█████████▌| 286/300 [01:37<00:03,  3.58epoch/s, loss=0.00118, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 186.24batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|█████████▌| 287/300 [01:37<00:03,  3.65epoch/s, loss=0.00118, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 179.25batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|█████████▌| 288/300 [01:37<00:03,  3.61epoch/s, loss=0.00116, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  50%|█████     | 17/34 [00:00<00:00, 168.12batch/s]\u001b[A\n",
      "Training epochs on cpu:  96%|█████████▋| 289/300 [01:38<00:03,  3.63epoch/s, loss=0.00118, prev_loss=0.00116]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 173.46batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|█████████▋| 290/300 [01:38<00:02,  3.62epoch/s, loss=0.00115, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  35%|███▌      | 12/34 [00:00<00:00, 110.09batch/s]\u001b[A\n",
      "Training batches on cpu:  85%|████████▌ | 29/34 [00:00<00:00, 143.26batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|█████████▋| 291/300 [01:38<00:02,  3.42epoch/s, loss=0.00114, prev_loss=0.00115]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 181.73batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|█████████▋| 292/300 [01:39<00:02,  3.42epoch/s, loss=0.00113, prev_loss=0.00114]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|█████████▊| 293/300 [01:39<00:02,  3.49epoch/s, loss=0.00117, prev_loss=0.00113]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 184.47batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|█████████▊| 294/300 [01:39<00:01,  3.58epoch/s, loss=0.00119, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 178.22batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|█████████▊| 295/300 [01:39<00:01,  3.61epoch/s, loss=0.00113, prev_loss=0.00119]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  44%|████▍     | 15/34 [00:00<00:00, 146.88batch/s]\u001b[A\n",
      "Training batches on cpu:  91%|█████████ | 31/34 [00:00<00:00, 154.50batch/s]\u001b[A\n",
      "Training epochs on cpu:  99%|█████████▊| 296/300 [01:40<00:01,  3.52epoch/s, loss=0.00117, prev_loss=0.00113]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  53%|█████▎    | 18/34 [00:00<00:00, 175.54batch/s]\u001b[A\n",
      "Training epochs on cpu:  99%|█████████▉| 297/300 [01:40<00:00,  3.56epoch/s, loss=0.00118, prev_loss=0.00117]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 181.02batch/s]\u001b[A\n",
      "Training epochs on cpu:  99%|█████████▉| 298/300 [01:40<00:00,  3.63epoch/s, loss=0.00115, prev_loss=0.00118]\n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  56%|█████▌    | 19/34 [00:00<00:00, 187.76batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|█████████▉| 299/300 [01:41<00:00,  3.67epoch/s, loss=0.0012, prev_loss=0.00115] \n",
      "Training batches on cpu:   0%|          | 0/34 [00:00<?, ?batch/s]\u001b[A\n",
      "Training batches on cpu:  41%|████      | 14/34 [00:00<00:00, 136.35batch/s]\u001b[A\n",
      "Training batches on cpu:  94%|█████████▍| 32/34 [00:00<00:00, 161.15batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|██████████| 300/300 [01:41<00:00,  2.96epoch/s, loss=0.00114, prev_loss=0.0012]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 1.07k/1.07k [00:00<00:00, 5.12ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.26s seconds\n"
     ]
    }
   ],
   "source": [
    "# Базовый пайплайн обучения модели\n",
    "from pykeen.pipeline import pipeline\n",
    "result = pipeline(\n",
    "    model='TransE',  # строка или класс модели\n",
    "    dataset='kinships', # строка или класс а датасетом\n",
    "    training_kwargs={'num_epochs':300}  # аргументы тренировочного цикла\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdB0DccpfVD1"
   },
   "source": [
    "Основные метрики для оценки предсказательных способностей KGE моделей можно извлечь из объекта-результата. Полный лист включает optimistic, pessimistic, realistic метрики для каждой части head / tail prediction. ([документация](https://pykeen.readthedocs.io/en/stable/tutorial/understanding_evaluation.html)). Оценка, по умолчанию, в фильрованном режиме (filtered setting). Как правило, нас интересует агрегированный результат по обеим частям - для этого мы будем извлекать:\n",
    "* both - агрегация head / tail предсказаний\n",
    "* realistic - реалистичный сценарний оценки \n",
    "\n",
    "Метрики:\n",
    "* `hits_at_1` , `hits_at_3`, `hits_at_5`, `hits_at_10`\n",
    "* `inverse_harmonic_mean_rank` -  он же [Mean Reciprocal Rank (MRR)](https://pykeen.readthedocs.io/en/stable/tutorial/understanding_evaluation.html#inverse-geometric-mean-rank) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VQ8edlrHIFEQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.19941009578314128\n",
      "Hits@1 : 0.016294227188081937\n",
      "Hits@3 : 0.24394785847299813\n",
      "Hits@5 : 0.39664804469273746\n",
      "Hits@10 : 0.633147113594041\n"
     ]
    }
   ],
   "source": [
    "# весь список метрик доступен через to_flat_dict()\n",
    "# from pprint import pprint\n",
    "# pprint(result.metric_results.to_flat_dict())\n",
    "print(f\"MRR: {result.metric_results.to_flat_dict()['both.realistic.inverse_harmonic_mean_rank']}\")\n",
    "for k in [1,3,5,10]:\n",
    "    print(f\"Hits@{k} : {result.metric_results.to_flat_dict()['both.realistic.hits_at_'+str(k)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YReoAHRFPEZH"
   },
   "source": [
    "## Имплементация собственных моделей\n",
    "\n",
    "Собственные модели можно создать двумя способами:\n",
    "1. Наследовать класс `ERModel` или `EntityRelationEmbeddingModel` в зависимости от выбранного алгоритма. [Документация](https://pykeen.readthedocs.io/en/stable/extending/models_new.html)\n",
    "2. Создать [interaction function](https://pykeen.readthedocs.io/en/stable/byo/interaction.html) и сгенерировать класс модели через него \n",
    "\n",
    "Каждая модель должна переопределить один (или три) метода:\n",
    "\n",
    "```\n",
    "def score_hrt(hrt_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "    # hrt_batch: триплеты [batch_size, 3], где 0 - субъекты, 1 - предикаты, 2 - объекты\n",
    "    # output: [batch_size, 1]\n",
    "    # оценивание всего триплета, стандарт для sLCWA тренировки\n",
    "    # получение эмбеддингов\n",
    "    # подсчет score function / interaction function\n",
    "```\n",
    "\n",
    "```\n",
    "def score_t(hr_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "    # hr_batch: триплеты [batch_size, 2], где 0 - субъекты, 1 - предикаты\n",
    "    # output: [batch_size, num_entities]\n",
    "    # оценивание всех возможных комбинаций с объектами (h,r, ?), стандарт для LCWA тренировки и инференса\n",
    "    # получение эмбеддингов\n",
    "    # подсчет score function / interaction function\n",
    "```\n",
    "\n",
    "```\n",
    "def score_h(rt_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "    # rt_batch: триплеты [batch_size, 2], где 0 - предикаты, 1 - объекты\n",
    "    # output: [batch_size, num_entities]\n",
    "    # оценивание всех возможных комбинаций с субьъектами (?,r,t), стандарт для LCWA тренировки и инференса без инверсных триплетов\n",
    "    # получение эмбеддингов\n",
    "    # подсчет score function / interaction function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlfhoToN5fgp"
   },
   "source": [
    "### Базовый класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TvyBxwFyPHwV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pykeen\n",
    "from pykeen.models import ERModel, EntityRelationEmbeddingModel\n",
    "\n",
    "# Базовый класс для моделей в этом упражнении\n",
    "class CustomModel(EntityRelationEmbeddingModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # When defining your class, any hyper-parameters that can be configured should be\n",
    "        # made as arguments to the __init__() function. When running the pipeline(), these\n",
    "        # are passed via the ``model_kwargs``.\n",
    "        embedding_dim: int = 50,\n",
    "        # All remaining arguments are simply passed through to the parent constructor. If you\n",
    "        # want access to them, you can name them explicitly. See the pykeen.models.ERModel\n",
    "        # documentation for a full list\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # since this is a python class, you can feel free to get creative here. One example of\n",
    "        # pre-processing is to derive the shape for the relation representation based on the\n",
    "        # embedding dimension.\n",
    "        super().__init__(\n",
    "            # Define the entity representations using the EmbeddingSpecification. By default, each\n",
    "            # embedding is linear. You can use the ``shape`` kwarg to specify higher dimensional\n",
    "            # tensor shapes.\n",
    "            entity_representations=pykeen.nn.EmbeddingSpecification(\n",
    "                embedding_dim=embedding_dim,\n",
    "            ),\n",
    "            # Define the relation representations the same as the entities\n",
    "            relation_representations=pykeen.nn.EmbeddingSpecification(\n",
    "                embedding_dim=embedding_dim,\n",
    "            ),\n",
    "            # All other arguments are passed through, such as the ``triples_factory``, ``loss``,\n",
    "            # ``preferred_device``, and others. These are all handled by the pipeline() function\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzZGGT020a-D"
   },
   "source": [
    "### TransE\n",
    "\n",
    "Реализуйте `CustomTransE` по стандартной формуле: \n",
    "$$ \n",
    "f(h, r, t) = - \\|\\textbf{e}_h + \\textbf{e}_r - \\textbf{e}_t\\|_{p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VfB8bVHtNN14"
   },
   "outputs": [],
   "source": [
    "class CustomTransE(CustomModel):\n",
    "    \n",
    "    # для sLCWA сценария\n",
    "    def score_hrt(self, hrt_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 3], где 0 - субъекты, 1 - предикаты, 2 - объекты\n",
    "        # output: [batch_size, 1]\n",
    "        # получение эмбеддингов\n",
    "        h = self.entity_embeddings(indices=hrt_batch[:, 0])\n",
    "        r = self.relation_embeddings(indices=hrt_batch[:, 1])\n",
    "        t = self.entity_embeddings(indices=hrt_batch[:, 2])\n",
    "\n",
    "        # подсчет scoring function\n",
    "        # пусть норма p=1\n",
    "        return -1 * torch.norm(h+r-t, p=1)\n",
    "\n",
    "        \n",
    "        #### \n",
    "        \n",
    "\n",
    "    # для LCWA сценария\n",
    "    def score_t(self, hr_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 2], где 0 - субъекты, 1 - предикаты\n",
    "        # output: [batch_size, num_entities]\n",
    "        # получение эмбеддингов\n",
    "        h = self.entity_embeddings(indices=hr_batch[:, 0])\n",
    "        r = self.relation_embeddings(indices=hr_batch[:, 1])\n",
    "        t = self.entity_embeddings(indices=None)  # None - получение всей матрицы\n",
    "\n",
    "        # подсчет scoring function для всех объектов (tails)\n",
    "        # пусть норма p=1\n",
    "        # будьте аккуратны с tensor shapes\n",
    "        return -1 * torch.norm(h+r-t, p=1)\n",
    "\n",
    "        \n",
    "        #### \n",
    "    \n",
    "    def score_h(self, rt_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 2], где 0 - предикаты, 1 - объекты\n",
    "        # output: [batch_size, num_entities]\n",
    "        # получение эмбеддингов\n",
    "        h = self.entity_embeddings(indices=None)\n",
    "        r = self.relation_embeddings(indices=rt_batch[:, 0])\n",
    "        t = self.entity_embeddings(indices=rt_batch[:, 1])  # None - получение всей матрицы\n",
    "\n",
    "        # подсчет scoring function для всех субъектов (heads)\n",
    "        # пусть норма p=1\n",
    "        # будьте аккуратны с tensor shapes\n",
    "        return -1 * torch.norm(h+r-t, p=1)\n",
    "\n",
    "        \n",
    "        #### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsj3Xfxu5jio"
   },
   "source": [
    "Проверьте работоспособность модели в sLCWA сценарии. Loss @ 50 epochs ~ 0.0106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MgAxqUACN8-8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.datasets.utils:Loading cached preprocessed dataset from file:///C:/Users/El%20Tuko/.data/pykeen/datasets/kinships/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/El%20Tuko/.data/pykeen/datasets/kinships/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/training\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/El%20Tuko/.data/pykeen/datasets/kinships/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/testing\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/El%20Tuko/.data/pykeen/datasets/kinships/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/validation\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pykeen.nn' has no attribute 'EmbeddingSpecification'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_slcwa \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCustomTransE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkinships\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_loop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mslcwa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1603073093\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pykeen\\lib\\site-packages\\pykeen\\pipeline\\api.py:1079\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(dataset, dataset_kwargs, training, testing, validation, evaluation_entity_whitelist, evaluation_relation_whitelist, model, model_kwargs, interaction, interaction_kwargs, dimensions, loss, loss_kwargs, regularizer, regularizer_kwargs, optimizer, optimizer_kwargs, clear_optimizer, lr_scheduler, lr_scheduler_kwargs, training_loop, training_loop_kwargs, negative_sampler, negative_sampler_kwargs, epochs, training_kwargs, stopper, stopper_kwargs, evaluator, evaluator_kwargs, evaluation_kwargs, result_tracker, result_tracker_kwargs, metadata, device, random_seed, use_testing_data, evaluation_fallback, filter_validation_when_testing, use_tqdm)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     model_instance \u001b[38;5;241m=\u001b[39m cast(Model, model)\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;66;03m# TODO should training be reset?\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# TODO should kwargs for loss and regularizer be checked and raised for?\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1079\u001b[0m     model_instance, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_build_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregularizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregularizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_random_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_random_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_triples_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1091\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mto(_device)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# Log model parameters\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pykeen\\lib\\site-packages\\pykeen\\pipeline\\api.py:791\u001b[0m, in \u001b[0;36m_build_model_helper\u001b[1;34m(model, model_kwargs, loss, loss_kwargs, _device, _random_seed, regularizer, regularizer_kwargs, training_triples_factory)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_name, param_default \u001b[38;5;129;01min\u001b[39;00m _get_model_defaults(model)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    788\u001b[0m         model_kwargs\u001b[38;5;241m.\u001b[39msetdefault(param_name, param_default)\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 791\u001b[0m     \u001b[43mmodel_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriples_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_triples_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    796\u001b[0m     model_kwargs,\n\u001b[0;32m    797\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pykeen\\lib\\site-packages\\class_resolver\\api.py:204\u001b[0m, in \u001b[0;36mClassResolver.make\u001b[1;34m(self, query, pos_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mcls\u001b[39m: Type[X] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup(query)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpos_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired keyword-only argument\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pykeen\\lib\\site-packages\\pykeen\\models\\base.py:824\u001b[0m, in \u001b[0;36m_add_post_reset_parameters.<locals>._new_init\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(_original_init)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_new_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 824\u001b[0m     \u001b[43m_original_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters_()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pykeen\\lib\\site-packages\\pykeen\\models\\base.py:824\u001b[0m, in \u001b[0;36m_add_post_reset_parameters.<locals>._new_init\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(_original_init)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_new_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 824\u001b[0m     \u001b[43m_original_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters_()\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mCustomModel.__init__\u001b[1;34m(self, embedding_dim, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# When defining your class, any hyper-parameters that can be configured should be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# pre-processing is to derive the shape for the relation representation based on the\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# embedding dimension.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# Define the entity representations using the EmbeddingSpecification. By default, each\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# embedding is linear. You can use the ``shape`` kwarg to specify higher dimensional\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# tensor shapes.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m         entity_representations\u001b[38;5;241m=\u001b[39m\u001b[43mpykeen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingSpecification\u001b[49m(\n\u001b[0;32m     26\u001b[0m             embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[0;32m     27\u001b[0m         ),\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Define the relation representations the same as the entities\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         relation_representations\u001b[38;5;241m=\u001b[39mpykeen\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mEmbeddingSpecification(\n\u001b[0;32m     30\u001b[0m             embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[0;32m     31\u001b[0m         ),\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# All other arguments are passed through, such as the ``triples_factory``, ``loss``,\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# ``preferred_device``, and others. These are all handled by the pipeline() function\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     35\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pykeen.nn' has no attribute 'EmbeddingSpecification'"
     ]
    }
   ],
   "source": [
    "result_slcwa = pipeline(\n",
    "    model=CustomTransE,\n",
    "    dataset='kinships',\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='slcwa',\n",
    "    random_seed=1603073093,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaz91aPu73Y_"
   },
   "source": [
    "Проверьте работоспособность модели в LCWA сценарии c инверсными гранями. Loss @ 50 epochs ~ 0.0102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn9iLBXm70aE"
   },
   "outputs": [],
   "source": [
    "result_lcwa = pipeline(\n",
    "    model=CustomTransE,\n",
    "    dataset='kinships',\n",
    "    dataset_kwargs={'create_inverse_triples': True},\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='lcwa',\n",
    "    random_seed=1603073093,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJnL8y__9Jm5"
   },
   "source": [
    "Теперь поэкспериментируйте с `num_epochs=500` и разными размерностями `model_kwargs` / `embedding_dim` \n",
    "* Натренируйте пайплайны с embedding dimension={50,100,200,500}\n",
    "* Постройте график зависимости MRR от embedding dimension с двумя моделями (sLCWA и LCWA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6QYHByiU-uh"
   },
   "outputs": [],
   "source": [
    "### Построение графика из результатов пайплайнов ###\n",
    "### Используйте любые стандартные библиотеки графиков (matplotlib, seaborn)\n",
    "### ВАШ КОД ЗДЕСЬ ###\n",
    "\n",
    "\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObUymB3t-eTn"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLebeU_u-fB4"
   },
   "source": [
    "### DistMult\n",
    "\n",
    "Реализуйте `CustomDistMult` по стандартной формуле: \n",
    "$$ \n",
    "f(h, r, t) =  \\|\\textbf{e}_h \\odot \\textbf{e}_r \\odot \\textbf{e}_t\\|_{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baKNCPt1-fB4"
   },
   "outputs": [],
   "source": [
    "class CustomDistMult(CustomModel):\n",
    "    \n",
    "    # для sLCWA сценария\n",
    "    def score_hrt(self, hrt_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 3], где 0 - субъекты, 1 - предикаты, 2 - объекты\n",
    "        # output: [batch_size, 1]\n",
    "        # получение эмбеддингов\n",
    "        h = self.entity_embeddings(indices=hrt_batch[:, 0])\n",
    "        r = self.relation_embeddings(indices=hrt_batch[:, 1])\n",
    "        t = self.entity_embeddings(indices=hrt_batch[:, 2])\n",
    "\n",
    "        # подсчет scoring function\n",
    "        # вместо нормы можно использовать сумму произведения\n",
    "        #### ВАШ КОД (1-2 строки) ####\n",
    "\n",
    "\n",
    "        #### \n",
    "        \n",
    "\n",
    "    # для LCWA сценария\n",
    "    def score_t(self, hr_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 2], где 0 - субъекты, 1 - предикаты\n",
    "        # output: [batch_size, num_entities]\n",
    "        # получение эмбеддингов\n",
    "        h = self.entity_embeddings(indices=hr_batch[:, 0])\n",
    "        r = self.relation_embeddings(indices=hr_batch[:, 1])\n",
    "        t = self.entity_embeddings(indices=None)  # None - получение всей матрицы\n",
    "\n",
    "        # подсчет scoring function для всех объектов (tails)\n",
    "        # вместо нормы можно использовать сумму произведения\n",
    "        # будьте аккуратны с tensor shapes\n",
    "        #### ВАШ КОД (1-2 строки) ####\n",
    "\n",
    "        \n",
    "        #### \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdxEgI3I-fB5"
   },
   "source": [
    "Проверьте работоспособность модели в sLCWA сценарии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_NvNBqg-fB5"
   },
   "outputs": [],
   "source": [
    "result_slcwa = pipeline(\n",
    "    model=CustomDistMult,\n",
    "    dataset='kinships',\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='slcwa',\n",
    "    random_seed=1603073093,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2I9d6xq-fB6"
   },
   "source": [
    "Проверьте работоспособность модели в LCWA сценарии c инверсными гранями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqxOAVWa-fB6"
   },
   "outputs": [],
   "source": [
    "result_lcwa = pipeline(\n",
    "    model=CustomDistMult,\n",
    "    dataset='kinships',\n",
    "    dataset_kwargs={'create_inverse_triples': True},\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='lcwa',\n",
    "    random_seed=1603073093,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuZNtCxJlFD4"
   },
   "source": [
    "### ComplEx\n",
    "\n",
    "Реализуйте `CustomComplEx` по стандартной формуле: \n",
    "$$ \n",
    "f(h, r, t) =  \\text{Re} (\\textbf{e}_h \\odot \\textbf{e}_r \\odot \\bar{\\textbf{e}}_t) \n",
    "$$\n",
    "\n",
    "где $\\bar{\\mathbf{e}}_t$ - комплексно сопряженное от исходного $\\mathbf{e}_t$.\n",
    "\n",
    "Каждый вектор сущностей и предикатов теперь представляется конкатенацией действительной Re(x) и мнимой Im(x) частей. Поэтому после получения соответствующего вектора из эмбеддинг таблицы его следует разделить на два (например через `torch.chunk()`).\n",
    "\n",
    "Упраженение на получение конкретной реализации после раскрытия скобок - одно из ДЗ Лекции 7, и здесь можно применить этот результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2YX9Ba_lFD4"
   },
   "outputs": [],
   "source": [
    "class CustomComplEx(CustomModel):\n",
    "\n",
    "    # для упрощения подсчетов можно упаковать взаимодействие в функцию\n",
    "    def complex_interaction(self, \n",
    "                            h_re: torch.FloatTensor,\n",
    "                            h_im: torch.FloatTensor,\n",
    "                            r_re: torch.FloatTensor,\n",
    "                            r_im: torch.FloatTensor,\n",
    "                            t_re: torch.FloatTensor,\n",
    "                            t_im: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # реализация формулы \n",
    "         \n",
    "    \n",
    "    # для sLCWA сценария\n",
    "    def score_hrt(self, hrt_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 3], где 0 - субъекты, 1 - предикаты, 2 - объекты\n",
    "        # output: [batch_size, 1]\n",
    "        # получение эмбеддингов\n",
    "        h_re, h_im = self.entity_embeddings(indices=hrt_batch[:, 0]).chunk(2, dim=-1)\n",
    "        r_re, r_im = self.relation_embeddings(indices=hrt_batch[:, 1]).chunk(2, dim=-1)\n",
    "        t_re, t_im = self.entity_embeddings(indices=hrt_batch[:, 2]).chunk(2, dim=-1)\n",
    "\n",
    "        # подсчет scoring function\n",
    "        # используйте выведенную формулу из ДЗ к Лекции 7\n",
    "        #### ВАШ КОД (5-8 строк) ####\n",
    "\n",
    "\n",
    "        #### \n",
    "        \n",
    "\n",
    "    # для LCWA сценария\n",
    "    def score_t(self, hr_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 2], где 0 - субъекты, 1 - предикаты\n",
    "        # output: [batch_size, num_entities]\n",
    "        # получение эмбеддингов\n",
    "        h_re, h_im = self.entity_embeddings(indices=hr_batch[:, 0]).chunk(2, dim=-1)\n",
    "        r_re, r_im = self.relation_embeddings(indices=hr_batch[:, 1]).chunk(2, dim=-1)\n",
    "        t_re, t_im = self.entity_embeddings(indices=None).chunk(2, dim=-1)  # None - получение всей матрицы\n",
    "\n",
    "        # подсчет scoring function для всех объектов (tails)\n",
    "        # используйте выведенную формулу из ДЗ к Лекции 7\n",
    "        # будьте аккуратны с tensor shapes\n",
    "        #### ВАШ КОД (5-8 строк) ####\n",
    "\n",
    "        \n",
    "        #### \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTmAapYPlFD5"
   },
   "source": [
    "Проверьте работоспособность модели в sLCWA сценарии. Размерность комплексных моделей включает действительную и мнимую части, ее можно менять через `model_kwargs` / `embedding_dim`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY5yYUatlFD5"
   },
   "outputs": [],
   "source": [
    "result_slcwa = pipeline(\n",
    "    model=CustomComplEx,\n",
    "    model_kwarags={'embedding_dim': 200},\n",
    "    dataset='kinships',\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='slcwa',\n",
    "    random_seed=1603073093,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4A9rtOflFD5"
   },
   "source": [
    "Проверьте работоспособность модели в LCWA сценарии c инверсными гранями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViTw63XmlFD5"
   },
   "outputs": [],
   "source": [
    "result_lcwa = pipeline(\n",
    "    model=CustomComplEx,\n",
    "    model_kwarags={'embedding_dim': 200},\n",
    "    dataset='kinships',\n",
    "    dataset_kwargs={'create_inverse_triples': True},\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='lcwa',\n",
    "    random_seed=1603073093,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNWbdVDzxRuL"
   },
   "source": [
    "### RotatE\n",
    "\n",
    "Реализуйте `CustomRotatE` по стандартной формуле: \n",
    "$$ \n",
    "f(h, r, t) =  - \\| h \\odot r - t \\|\n",
    "$$\n",
    "\n",
    "где $h \\odot r$ - вращение вектора в комплексном пространстве.\n",
    "\n",
    "Каждый вектор сущностей и предикатов теперь представляется конкатенацией действительной Re(x) и мнимой Im(x) частей. Поэтому после получения соответствующего вектора из эмбеддинг таблицы его следует разделить на два (например через `torch.chunk()`).\n",
    "\n",
    "Помимо этого, векторы предикатов обозначают части оператора вращения. Мы используем дополнительные `initializer` и `complex_normalize` чтобы нормировать модуль комплексного числа к единице.\n",
    "\n",
    "Упраженение на получение конкретной реализации комплексного вращения - одно из ДЗ Лекции 7, и здесь можно применить этот результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYCb_0VZxRud"
   },
   "outputs": [],
   "source": [
    "from pykeen.nn.init import init_phases, xavier_uniform_\n",
    "from pykeen.nn import EmbeddingSpecification\n",
    "from pykeen.typing import Constrainer, Hint, Initializer\n",
    "from pykeen.utils import complex_normalize\n",
    "\n",
    "class CustomRotatE(CustomModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        embedding_dim: int = 200,\n",
    "        entity_initializer: Hint[Initializer] = xavier_uniform_,\n",
    "        relation_initializer: Hint[Initializer] = init_phases,\n",
    "        relation_constrainer: Hint[Constrainer] = complex_normalize,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            entity_representations=EmbeddingSpecification(\n",
    "                embedding_dim=embedding_dim,\n",
    "                initializer=entity_initializer,\n",
    "                dtype=torch.cfloat,\n",
    "            ),\n",
    "            relation_representations=EmbeddingSpecification(\n",
    "                embedding_dim=embedding_dim,\n",
    "                initializer=relation_initializer,\n",
    "                constrainer=relation_constrainer,\n",
    "                dtype=torch.cfloat,\n",
    "            ),\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.real_embedding_dim = embedding_dim\n",
    "\n",
    "    # для упрощения подсчетов можно упаковать взаимодействие в функцию\n",
    "    def rotate_interaction(self, \n",
    "                            h_re: torch.FloatTensor,\n",
    "                            h_im: torch.FloatTensor,\n",
    "                            r_re: torch.FloatTensor,\n",
    "                            r_im: torch.FloatTensor,\n",
    "                            t_re: torch.FloatTensor,\n",
    "                            t_im: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # реализация формулы \n",
    "                 \n",
    "    \n",
    "    # для sLCWA сценария\n",
    "    def score_hrt(self, hrt_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 3], где 0 - субъекты, 1 - предикаты, 2 - объекты\n",
    "        # output: [batch_size, 1]\n",
    "        # получение эмбеддингов\n",
    "        h_re, h_im = self.entity_embeddings(indices=hrt_batch[:, 0]).chunk(2, dim=-1)\n",
    "        r_re, r_im = self.relation_embeddings(indices=hrt_batch[:, 1]).chunk(2, dim=-1)\n",
    "        t_re, t_im = self.entity_embeddings(indices=hrt_batch[:, 2]).chunk(2, dim=-1)\n",
    "\n",
    "        # подсчет scoring function\n",
    "        # используйте выведенную формулу из ДЗ к Лекции 7\n",
    "        # пусть норма p=1\n",
    "        #### ВАШ КОД (5-8 строк) ####\n",
    "        \n",
    "\n",
    "        #### \n",
    "        \n",
    "\n",
    "    # для LCWA сценария\n",
    "    def score_t(self, hr_batch: torch.LongTensor) -> torch.FloatTensor:\n",
    "        # hrt_batch: триплеты [batch_size, 2], где 0 - субъекты, 1 - предикаты\n",
    "        # output: [batch_size, num_entities]\n",
    "        # получение эмбеддингов\n",
    "        h_re, h_im = self.entity_embeddings(indices=hr_batch[:, 0]).chunk(2, dim=-1)\n",
    "        r_re, r_im = self.relation_embeddings(indices=hr_batch[:, 1]).chunk(2, dim=-1)\n",
    "        t_re, t_im = self.entity_embeddings(indices=None).chunk(2, dim=-1)  # None - получение всей матрицы\n",
    "\n",
    "        # подсчет scoring function для всех объектов (tails)\n",
    "        # используйте выведенную формулу из ДЗ к Лекции 7\n",
    "        # пусть норма p=1\n",
    "        # будьте аккуратны с tensor shapes\n",
    "        #### ВАШ КОД (5-8 строк) ####\n",
    "        \n",
    "        \n",
    "        #### \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPa6o2DSxRud"
   },
   "source": [
    "Проверьте работоспособность модели в sLCWA сценарии. Размерность комплексных моделей включает действительную и мнимую части, ее можно менять через `model_kwargs` / `embedding_dim`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3XHTqYqxRud"
   },
   "outputs": [],
   "source": [
    "result_slcwa = pipeline(\n",
    "    model=CustomRotatE,\n",
    "    model_kwarags={'embedding_dim': 200},\n",
    "    dataset='kinships',\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='slcwa',\n",
    "    random_seed=1603073093,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgXqGL3axRud"
   },
   "source": [
    "Проверьте работоспособность модели в LCWA сценарии c инверсными гранями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rb6sguMwxRud"
   },
   "outputs": [],
   "source": [
    "result_lcwa = pipeline(\n",
    "    model=CustomRotatE,\n",
    "    model_kwarags={'embedding_dim': 200},\n",
    "    dataset='kinships',\n",
    "    dataset_kwargs={'create_inverse_triples': True},\n",
    "    training_kwargs={'num_epochs':50},\n",
    "    training_loop='lcwa',\n",
    "    random_seed=1603073093,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_03ToVLzJDs"
   },
   "source": [
    "### Визуализация\n",
    "\n",
    "Используйте pipeline чтобы натренировать несколько моделей в разных настройках `embedding_dim`, `num_epochs` и др. \n",
    "Визуализируйте полученные эмбеддинги (полной размерности для TransE и DistMult, и только векторы действительной части комплексного числа для ComplEx и RotatE) с помощью стандартного sklearn tSNE - находите ли вы кластеры сущностей?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBGeO5CJDRpI"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# получение матрицы эмбеддингов из результата тренировки\n",
    "# <RESULT_OBJ>.model.entity_embeddings().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFBO3pQzEvFU"
   },
   "outputs": [],
   "source": [
    "z = <YOUR_RESULT_OBJ>.model.entity_embeddings().cpu().detach().numpy()  # используйте слайсинг для получения первой половины колонок матрицы в случае комплексных чисел\n",
    "xs, ys = zip(*TSNE().fit_transform(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC7gtv1LHCk5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(xs, ys)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L7_Homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pykeen",
   "language": "python",
   "name": "pykeen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
