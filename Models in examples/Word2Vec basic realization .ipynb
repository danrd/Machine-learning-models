{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Из курса \"Нейронные сети и обработка текста\" со Stepic\n",
    "Word2Vec Skip Gram Negative Sampling\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate):\n",
    "    \"\"\"\n",
    "    genetate samples with ns_rate negative samples for the positive\n",
    "    \n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "\n",
    "    returns list of training samples (CenterWord, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    text_l = len(text)\n",
    "    text = text + 1 # смещаем индексы слов для удобства работы с маской\n",
    "    samples = []\n",
    "    radius = window_size // 2\n",
    "    \n",
    "    # создаем маску для отбора слов за счет матриц с соответствующими смещениями по диагоналям\n",
    "    mask_left = sum([np.diag(np.repeat(1, text_l + i), k=i) for i in range(-radius, 0)])\n",
    "    mask_right = sum([np.diag(np.repeat(1, text_l - i), k=i) for i in range(1, radius + 1)])  \n",
    "    mask = mask_left + mask_right\n",
    "    \n",
    "    # подсчитываем число позитивных примеров для расчета числа негативных и отбираем индексы слов\n",
    "    pos_num = np.sum(mask, axis=0)\n",
    "    positives = text * mask\n",
    "    positives = np.array(positives[positives != 0]) - 1\n",
    "    text = text - 1\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < text_l:\n",
    "        for _ in range(pos_num[i]):\n",
    "            samples.append([text[i], positives[j], 1])\n",
    "            j += 1    \n",
    "        for _ in range(ns_rate * pos_num[i]):\n",
    "            random_number = np.random.randint(0, text_l - 1)\n",
    "            if random_number == i:\n",
    "                random_number = text_l - 1 - random_number # если попадаем в центральное слово, то отражаем индекс\n",
    "            samples.append([text[i], text[random_number], 0])\n",
    "        i += 1\n",
    "    return np.array(samples)    \n",
    "    \n",
    "    \n",
    "def update_w2v_weights(center_embeddings, context_embeddings, center_word, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    update center_embeddings and context_embeddings inplace\n",
    "    \n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_word - int - identifier of center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    center = center_embeddings[center_word]\n",
    "    context = context_embeddings[context_word]\n",
    "    \n",
    "    probability = 1/(1 + np.exp(-sum(center * context)))\n",
    "    \n",
    "    der_w = (probability - label) * context\n",
    "    der_d = (probability - label) * center\n",
    "    \n",
    "    center_emb_new = center - learning_rate * der_w\n",
    "    context_emb_new = context - learning_rate * der_d    \n",
    "    \n",
    "    center_embeddings[center_word] = center_emb_new\n",
    "    context_embeddings[context_word] = context_emb_new\n",
    "    \n",
    "    return center_embeddings, context_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Из курса \"Нейронные сети и обработка текста\" со Stepic\n",
    "FastText SkipGram Negative Sampling \n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_list():\n",
    "    return ast.literal_eval(sys.stdin.readline())\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_ft_sgns_samples(text, window_size, vocab_size, ns_rate, token2subwords):\n",
    "    \"\"\"\n",
    "    genetate samples with ns_rate negative samples for the positive    \n",
    "    \n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "    token2subwords - list of lists of int - i-th sublist contains list of identifiers of n-grams for token #i (list of subword units)\n",
    "\n",
    "    returns list of training samples (CenterSubwords, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    text_l = len(text)\n",
    "    text = text + 1\n",
    "    samples = []\n",
    "    radius = (window_size - 1) // 2\n",
    "    \n",
    "    if radius >= text_l - 1: # если текст меньше окна, то вручную создаем позитивные примеры\n",
    "        pos_num = np.repeat(text_l - 1, text_l)\n",
    "        positives = np.hstack([list(text[0:i]) + list(text[i+1:]) for i in range(text_l)]) \n",
    "        positives = positives - 1\n",
    "    else:     \n",
    "        # создаем маску для отбора слов за счет матриц с соответствующими смещениями по диагоналям       \n",
    "        mask_left = sum([np.diag(np.repeat(1, text_l + i), k=i) for i in range(-radius, 0)])\n",
    "        mask_right = sum([np.diag(np.repeat(1, text_l - i), k=i) for i in range(1, radius + 1)])  \n",
    "        mask = mask_left + mask_right\n",
    "       \n",
    "        # подсчитываем число позитивных примеров для расчета числа негативных и отбираем индексы слов    \n",
    "        pos_num = np.sum(mask, axis=0)\n",
    "        positives = text * mask\n",
    "        positives = np.array(positives[positives != 0]) - 1\n",
    "    \n",
    "    text = text - 1\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < text_l:\n",
    "        for _ in range(pos_num[i]):\n",
    "            samples.append(tuple([[text[i]] + list(set(token2subwords[text[i]])), positives[j], 1]))\n",
    "            j += 1    \n",
    "        for _ in range(ns_rate * pos_num[i]):\n",
    "            random_number = np.random.randint(0, text_l - 1)\n",
    "            if random_number == i:\n",
    "                random_number = text_l - 1 - random_number\n",
    "            samples.append(tuple([[text[i]] + list(set(token2subwords[text[i]])), text[random_number], 0]))\n",
    "        i += 1\n",
    "    return samples\n",
    "\n",
    "def update_ft_weights(center_embeddings, context_embeddings, center_subwords, context_word, label, learning_rate):\n",
    "    \"\"\"\n",
    "    update center_embeddings, context_embeddings inplace\n",
    "    \n",
    "    center_embeddings - VocabSize x EmbSize\n",
    "    context_embeddings - VocabSize x EmbSize\n",
    "    center_subwords - list of ints - list of identifiers of n-grams contained in center word\n",
    "    context_word - int - identifier of context word\n",
    "    label - 1 if context_word is real, 0 if it is negative\n",
    "    learning_rate - float > 0 - size of gradient step\n",
    "    \"\"\"\n",
    "    center_l = len(center_subwords)\n",
    "    center = sum([center_embeddings[word] for word in center_subwords]) / center_l\n",
    "    context = context_embeddings[context_word]\n",
    "    \n",
    "    probability = 1 / (1 + np.exp(-sum(center * context)))\n",
    "    \n",
    "    der_w = ((probability-label) * context) / center_l\n",
    "    der_d = (probability-label) * center\n",
    "    \n",
    "    context_emb_new = context - learning_rate * der_d    \n",
    "    center_embeddings[center_subwords] +=  - learning_rate * der_w\n",
    "    \n",
    "    context_embeddings[context_word] = context_emb_new\n",
    "    return center_embeddings, context_embeddings    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
