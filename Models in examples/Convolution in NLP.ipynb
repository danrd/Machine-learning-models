{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "На основе курса \"Нейронные сети и обработка текста\" со Stepic\n",
    "Convolution funtions for text\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def apply_convolution(data, kernel, bias):\n",
    "    \"\"\"\n",
    "    returns array with shape OutLen x OutChannels after kernel usage   \n",
    "   \n",
    "    data - InLen x InChannels\n",
    "    kernel - OutChannels x InChannels x KernelSize\n",
    "    bias - OutChannels\n",
    "    \"\"\"\n",
    "    in_len, _ = data.shape\n",
    "    out_c, in_c, k = kernel.shape\n",
    "    out_len = in_len - k + 1\n",
    "    \n",
    "    data_vectorization = np.stack([data[0+i:k+i].T.flatten() for i in range(out_len)])\n",
    "    kernel = kernel.reshape(out_c, -1, 1)\n",
    "    \n",
    "    res = (data_vectorization@kernel).squeeze().T + bias.reshape(1, -1)\n",
    "    return res.reshape(out_len, out_c)\n",
    "\n",
    "def calculate_kernel_grad(x, y, kernel, bias):\n",
    "    \"\"\"\n",
    "    returns gradient for kernel with shape OutChannels x InChannels x KernelSize    \n",
    "    \n",
    "    x - InLen x InChannels\n",
    "    y - OutLen x OutChannels\n",
    "    kernel - OutChannels x InChannels x KernelSize\n",
    "    bias - OutChannels\n",
    "    \"\"\"\n",
    "    in_len, _ = x.shape \n",
    "    out_c, in_c, k = kernel.shape\n",
    "    out_len = in_len-k+1\n",
    "    \n",
    "    ind = np.arange(k)[None, :] + np.arange(in_len - k + 1)[:, None]\n",
    "    grad = np.sum(x[ind], axis=0).T\n",
    "    res = np.stack(repeat(grad, out_c))\n",
    "    return res\n",
    "\n",
    "def calculate_conv_x_grad(x, y, kernel, bias):\n",
    "    \"\"\"\n",
    "    returns gradient for x with shape InLen x InChannels   \n",
    "    \n",
    "    x - InLen x InChannels\n",
    "    y - OutLen x OutChannels\n",
    "    kernel - OutChannels x InChannels x KernelSize\n",
    "    bias - OutChannels\n",
    "    \"\"\"\n",
    "    in_l, in_c = x.shape\n",
    "    out_c, in_c, k = kernel.shape\n",
    "    kernel = np.sum(kernel, axis=0).T\n",
    "    \n",
    "    top = np.vstack([np.sum(kernel[0:i], axis=0) for i in range(1,k)])\n",
    "    bottom = np.vstack([np.sum(kernel[::-1][0:i], axis=0) for i in range(1,k)])[::-1]\n",
    "    \n",
    "    middle_part = np.sum(kernel, keepdims=True, axis=0)\n",
    "    middle = np.vstack(repeat(middle_part, in_l-(k-1)*2))\n",
    "   \n",
    "    grad = np.vstack((top, middle, bottom))\n",
    "    return grad\n",
    "\n",
    "def calculate_receptive_field(layers):\n",
    "    \"\"\"\n",
    "    returns int - receptive field size    \n",
    "\n",
    "    layers - list of LayerInfo\n",
    "    \"\"\"\n",
    "    layers = np.array(layers).T\n",
    "    kernels = np.array(layers[0])\n",
    "    dilations = np.array(layers[1])\n",
    "    percep_field = 1 + np.sum((kernels - 1) * dilations)\n",
    "    return percep_field\n",
    "\n",
    "def max_pooling(features, kernel_size):\n",
    "    \"\"\"\n",
    "    returns tuple of two matrices of shape OutLen x EmbSize:\n",
    "         - output features (main result)\n",
    "         - relative indices of maximum elements for each position of sliding window    \n",
    "    \n",
    "    features - InLen x EmbSize - features of elements of input sequence\n",
    "    kernel_size - positive integer - size of sliding window\n",
    "    \"\"\"\n",
    "    in_len, emb_size = features.shape\n",
    "    f = features.T # транспонируем признаки для удобства работы с numpy array\n",
    "    k = kernel_size\n",
    "    out_len = in_len - k + 1\n",
    "    \n",
    "    # шагаем по признакам ядром свертки, вытягивавем соответствующие элементы в строчку и сводим их вместе\n",
    "    vectorize_features = np.stack([f[i%emb_size][i//emb_size:i//emb_size+k].reshape(1, -1) for i in range(out_len * emb_size)])\n",
    "    #  после чего по каждой строке ищем максимум и соответствующий индекс \n",
    "    result = np.max(vectorize_features, keepdims=True, axis=2)\n",
    "    indices = np.where(vectorize_features==result)[-1].reshape(out_len, emb_size)\n",
    "    \n",
    "    result = result.squeeze().reshape(out_len, emb_size)\n",
    "    return result, indices\n",
    "\n",
    "def max_pooling_dldfeatures(features, kernel_size, indices, dldout):\n",
    "    \"\"\"\n",
    "    returns gradient for max pooling layer with shape InLen x EmbSize    \n",
    "    \n",
    "    features - InLen x EmbSize - features of elements of input sequence\n",
    "    kernel_size - positive integer - size of sliding window\n",
    "    indices - OutLen x EmbSize - relative indices of maximum elements for each window position\n",
    "    dldout - OutLen x EmbSize - partial derivative of loss function with respect to outputs of max_pooling layer\n",
    "    \"\"\"\n",
    "    \n",
    "    in_len, emb_size = features.shape\n",
    "    out_len, _ = dldout.shape\n",
    "    grad_init = np.zeros(features.shape).T\n",
    "    \n",
    "    for i, row in enumerate(indices.T):\n",
    "        for j in range(out_len):\n",
    "            grad_init[i][j+row[j]] += dldout.T[i][j]\n",
    "    return grad_init.T\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
