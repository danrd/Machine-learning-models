{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Из курса \"Нейронные сети и обработка текста\" со Stepic\n",
    "GloVe realization with sparse matrix\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "from itertools import permutations\n",
    "\n",
    "\n",
    "def read_array():\n",
    "    return ast.literal_eval(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def generate_coocurrence_matrix(texts, vocab_size):\n",
    "    \"\"\"\n",
    "    returns scipy.sparse.dok_matrix with words coocurrence statistic\n",
    "    \n",
    "    texts - list of lists of ints - i-th sublist contains identifiers of tokens in i-th document\n",
    "    vocab_size - int - size of vocabulary\n",
    "    \"\"\"\n",
    "    stat_matrix = dok_matrix((vocab_size, vocab_size), dtype=np.int_)\n",
    "    for doc in texts:\n",
    "        doc_u = list(set(doc))\n",
    "        pairs = permutations(doc_u, 2)\n",
    "        for pair in pairs:\n",
    "            stat_matrix[pair] += 1\n",
    "    return stat_matrix\n",
    "\n",
    "def update_glove_weights(x, w, d, alpha, max_x, learning_rate):\n",
    "    \"\"\"\n",
    "    inplace unpdating of glove weights\n",
    "    \n",
    "    x - square integer matrix VocabSize x VocabSize - coocurrence matrix\n",
    "    w - VocabSize x EmbSize - first word vectors\n",
    "    d - VocabSize x EmbSize - second word vectors\n",
    "    alpha - float - power in weight smoothing function f\n",
    "    max_x - int - maximum coocurrence count in weight smoothing function f\n",
    "    learning_rate - positive float - size of gradient step\n",
    "    \"\"\"\n",
    "    vocab_size, emb_size = w.shape\n",
    "    \n",
    "    x_lim = np.array(x)\n",
    "    x_lim[x_lim>max_x] = max_x\n",
    "    \n",
    "    f = (x_lim / max_x)**alpha\n",
    "    y = np.log1p(x)\n",
    "    \n",
    "    grad_w = (2 * f * (w@d.T - y))@d\n",
    "    grad_d = (2 * f * (w@d.T - y)).T@w\n",
    "    \n",
    "    w += -learning_rate * grad_w \n",
    "    d += -learning_rate * grad_d\n",
    "    return w, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest(embeddings, query_word_id, get_n):\n",
    "    \"\"\"\n",
    "    returns list of `get_n` tuples (word_id, similarity) sorted by descending order of similarity value \n",
    "    \n",
    "    embeddings - VocabSize x EmbSize - word embeddings\n",
    "    query_word_id - integer - id of query word to find most similar to\n",
    "    get_n - integer - number of most similar words to retrieve\n",
    "    \"\"\"\n",
    "    voc_size, emb_size = embeddings.shape\n",
    "    embeddings_normed = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    targer_word_emb = embeddings_normed[query_word_id]\n",
    "    similarity_list = np.stack([np.arange(voc_size), -np.linalg.norm(targer_word_emb-embeddings_normed, axis=1)]).T\n",
    "    result = sorted(similarity_list, reverse=True, key=lambda x: x[1])[0:get_n]\n",
    "    return result "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
